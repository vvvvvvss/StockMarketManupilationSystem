{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFhrmmHaueRK73lgm/RZTu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvvvvvss/StockMarketManupilationSystem/blob/main/Stock_Manupilation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real-time data processing and analysis"
      ],
      "metadata": {
        "id": "VKCdRf2Jj3Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install aiohttp pandas confluent-kafka"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_40dDFqZjYIv",
        "outputId": "cc139140-0640-4c0a-981c-69bfb25e8d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.14)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: confluent-kafka in /usr/local/lib/python3.11/dist-packages (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.18.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to see what kind of data can be fetched from the API: Alphavantage"
      ],
      "metadata": {
        "id": "Kehg7vij0PE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "API_KEY = \"QT13WY791JO16QMJ\"\n",
        "BASE_URL = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "def fetch_stock_data(symbol, interval=\"5min\"):\n",
        "    params = {\n",
        "        \"function\": \"TIME_SERIES_INTRADAY\",\n",
        "        \"symbol\": symbol,\n",
        "        \"interval\": interval,\n",
        "        \"apikey\": API_KEY,\n",
        "        \"outputsize\": \"compact\"\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"Error Message\" in data:\n",
        "        print(f\"Error fetching data for {symbol}: {data['Error Message']}\")\n",
        "        return None\n",
        "    elif f\"Time Series ({interval})\" in data:\n",
        "        time_series = data[f\"Time Series ({interval})\"]\n",
        "        df = pd.DataFrame.from_dict(time_series, orient=\"index\")\n",
        "        df.reset_index(inplace=True)\n",
        "        df.rename(columns={\"index\": \"timestamp\"}, inplace=True)\n",
        "        return df\n",
        "    else:\n",
        "        print(f\"Unexpected data format for {symbol}: {data}\")\n",
        "        return None\n",
        "\n",
        "stock_data = fetch_stock_data(\"AAPL\")\n",
        "if stock_data is not None:\n",
        "    print(stock_data.head())\n",
        "else:\n",
        "    print(\"Could not retrieve stock data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3muMVSN3PJ-",
        "outputId": "7b850295-0333-4042-ac88-769cfe39c548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             timestamp   1. open   2. high    3. low  4. close 5. volume\n",
            "0  2025-03-25 19:55:00  224.1300  224.3300  224.1000  224.2400      3316\n",
            "1  2025-03-25 19:50:00  224.2000  224.3300  224.0700  224.1000       738\n",
            "2  2025-03-25 19:45:00  224.1500  224.3300  224.0700  224.2800      2743\n",
            "3  2025-03-25 19:40:00  224.1500  224.1500  224.0700  224.0700       834\n",
            "4  2025-03-25 19:35:00  224.1000  224.1500  224.0500  224.0701       546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#    Data Collection - Rough\n",
        "\n",
        "1.   Fetch trading data from Alpha Vantage\n",
        "2.   Detect potential market manipulation using Isolation Forest\n",
        "3.   Mock implementation of social media sentiment collection\n",
        "\n"
      ],
      "metadata": {
        "id": "f3UoZNVs0owj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import requests\n",
        "\n",
        "class MarketManipulationDetector:\n",
        "    def __init__(self, alpha_vantage_key):\n",
        "        self.alpha_vantage_key = alpha_vantage_key\n",
        "        self.trading_data = None\n",
        "        self.sentiment_data = None\n",
        "\n",
        "    def fetch_trading_data(self, symbol):\n",
        "        url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&apikey={self.alpha_vantage_key}\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            raw_data = response.json()\n",
        "            self.trading_data = pd.DataFrame.from_dict(\n",
        "                raw_data.get('Time Series (Daily)', {}),\n",
        "                orient='index'\n",
        "            )\n",
        "            self.trading_data.columns = [\n",
        "                'open', 'high', 'low', 'close', 'volume'\n",
        "            ]\n",
        "            self.trading_data = self.trading_data.astype(float)\n",
        "\n",
        "    def detect_anomalous_trading(self):\n",
        "        if self.trading_data is None:\n",
        "            raise ValueError(\"Trading data not loaded\")\n",
        "\n",
        "\n",
        "        features = ['volume', 'close']\n",
        "        X = self.trading_data[features]\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        clf = IsolationForest(contamination=0.1, random_state=42)\n",
        "        y_pred = clf.fit_predict(X_scaled)\n",
        "        self.trading_data['is_anomaly'] = y_pred == -1\n",
        "\n",
        "        return self.trading_data[self.trading_data['is_anomaly']]\n",
        "\n",
        "    def collect_social_sentiment(self, symbol):\n",
        "        #  without StockTwits API\n",
        "\n",
        "        fake_sentiments = {\n",
        "            'bullish': 0.6,\n",
        "            'bearish': 0.3,\n",
        "            'neutral': 0.1\n",
        "        }\n",
        "        return fake_sentiments\n",
        "\n",
        "def main():\n",
        "\n",
        "    detector = MarketManipulationDetector(alpha_vantage_key='QT13WY791JO16QMJ')\n",
        "    detector.fetch_trading_data('INFY')\n",
        "\n",
        "    anomalies = detector.detect_anomalous_trading()\n",
        "    print(\"Potential Manipulative Trading Days:\")\n",
        "    print(anomalies)\n",
        "\n",
        "\n",
        "    sentiment = detector.collect_social_sentiment('INFY')\n",
        "    print(\"\\nSocial Media Sentiment:\")\n",
        "    print(sentiment)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDUXnDAq-ZR5",
        "outputId": "20aa68da-c4a7-4044-a4cb-19da435ccdde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Potential Manipulative Trading Days:\n",
            "             open    high      low  close      volume  is_anomaly\n",
            "2025-03-28  18.37  18.482  18.1050  18.17   7866062.0        True\n",
            "2025-03-27  18.70  18.780  18.5950  18.67   6249534.0        True\n",
            "2025-03-21  18.41  18.430  18.1700  18.32  18677618.0        True\n",
            "2025-03-20  18.33  18.390  17.9001  18.06  19376214.0        True\n",
            "2025-03-13  18.50  18.585  18.2600  18.29  10913566.0        True\n",
            "2025-03-12  18.49  18.645  18.3400  18.50  15292391.0        True\n",
            "2025-03-11  19.13  19.200  18.8100  18.97  17695135.0        True\n",
            "2025-01-16  22.60  22.600  21.3100  21.57  22922717.0        True\n",
            "2024-12-19  23.18  23.620  23.1000  23.42   9178696.0        True\n",
            "2024-12-13  23.52  23.630  23.2800  23.40   4443501.0        True\n",
            "\n",
            "Social Media Sentiment:\n",
            "{'bullish': 0.6, 'bearish': 0.3, 'neutral': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alpha_vantage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYjws_ZGAVhd",
        "outputId": "475a0c9f-2899-4afd-e7bd-59bd9c1aa6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting alpha_vantage\n",
            "  Downloading alpha_vantage-3.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from alpha_vantage) (3.11.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from alpha_vantage) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (2025.1.31)\n",
            "Downloading alpha_vantage-3.0.0-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: alpha_vantage\n",
            "Successfully installed alpha_vantage-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main code"
      ],
      "metadata": {
        "id": "sU4tfEFVzcG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "from textblob import TextBlob\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "\n",
        "\n",
        "#stock data collection\n",
        "ALPHA_VANTAGE_API_KEY = \"ED3T9IQN5OD495QC\"\n",
        "STOCK_SYMBOL = \"AAPL\"\n",
        "\n",
        "ts = TimeSeries(key=ALPHA_VANTAGE_API_KEY, output_format='pandas')\n",
        "data, meta_data = ts.get_daily(symbol=STOCK_SYMBOL, outputsize='compact')\n",
        "\n",
        "\n",
        "data.to_csv(\"stock_data.csv\") # storing stock data as a CSV file\n",
        "print(\"Stock data saved successfully.\")\n",
        "\n",
        "# StockTwits Data\n",
        "def fetch_stocktwits_data(symbol):\n",
        "    url = f\"https://api.stocktwits.com/api/2/streams/symbol/{symbol}.json\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def analyze_sentiment(messages):\n",
        "    sentiments = []\n",
        "    for msg in messages:\n",
        "        text = msg['body']\n",
        "        sentiment = TextBlob(text).sentiment.polarity\n",
        "        sentiments.append({'timestamp': msg['created_at'], 'text': text, 'sentiment_score': sentiment})\n",
        "    return sentiments\n",
        "\n",
        "stocktwits_data = fetch_stocktwits_data(\"TCS\")\n",
        "if stocktwits_data:\n",
        "    messages = stocktwits_data['messages']\n",
        "    sentiment_analysis = analyze_sentiment(messages)\n",
        "    df_sentiment = pd.DataFrame(sentiment_analysis)\n",
        "    df_sentiment.to_csv(\"sentiment_data.csv\", index=False)\n",
        "    print(\"Sentiment data saved successfully.\")\n",
        "else:\n",
        "    print(\"Failed to fetch StockTwits data.\")\n",
        "\n",
        "\n",
        "def analyze_news_sentiment(news_text):\n",
        "    return TextBlob(news_text).sentiment.polarity\n",
        "\n",
        "news_text_sample = \"Stock markets rally as tech stocks soar.\"\n",
        "print(\"Sample News Sentiment Score:\", analyze_news_sentiment(news_text_sample))\n"
      ],
      "metadata": {
        "id": "O2hbSfuShXEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36301a50-595c-4d54-9a35-06d167283768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock data saved successfully.\n",
            "Failed to fetch StockTwits data.\n",
            "Sample News Sentiment Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElDqIeTsr4b1",
        "outputId": "74a8e880-864f-41ee-cc1c-009b990ca01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=208730de486f11e99ae97bbc567df4a5a2853405ec2ad5fb060084893a8550ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection\n",
        "1. Fetch daily stock data using Alpha Vantage API\n",
        "2. Analyze sentiment of messages\n",
        "3. Analyze sentiment of news text\n",
        "\n"
      ],
      "metadata": {
        "id": "JUjm350A2Gko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "from textblob import TextBlob\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "import feedparser\n",
        "\n",
        "ALPHA_VANTAGE_API_KEY = \"ED3T9IQN5OD495QC\"\n",
        "STOCK_SYMBOL = \"AAPL\"\n",
        "STOCKTWITS_API_URL = \"https://api.stocktwits.com/api/2/streams/symbol/{symbol}.json\"\n",
        "\n",
        "def fetch_stock_data(symbol, api_key):\n",
        "    try:\n",
        "        ts = TimeSeries(key=api_key, output_format='pandas')\n",
        "        data, meta_data = ts.get_daily(symbol=symbol, outputsize='compact')\n",
        "        data.to_csv(\"stock_data.csv\")\n",
        "        print(f\"\\nStock data for {symbol} saved successfully.\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching stock data: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_stocktwits_data(symbol):\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        url = STOCKTWITS_API_URL.format(symbol=symbol)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            print(f\"Successfully fetched StockTwits data for {symbol}\")\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"Failed to fetch StockTwits data. Status code: {response.status_code}\")\n",
        "            print(f\"Response content: {response.text}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error in fetching StockTwits data: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_sentiment(messages):\n",
        "    sentiments = []\n",
        "    for msg in messages:\n",
        "        text = msg.get('body', '')\n",
        "        sentiment = TextBlob(text).sentiment.polarity\n",
        "        sentiments.append({\n",
        "            'timestamp': msg.get('created_at', 'N/A'),\n",
        "            'text': text,\n",
        "            'sentiment_score': sentiment\n",
        "        })\n",
        "    return sentiments\n",
        "\n",
        "def get_google_news_rss(stock_name):\n",
        "    url = f\"https://news.google.com/rss/search?q={stock_name}+stock\"\n",
        "    feed = feedparser.parse(url)\n",
        "\n",
        "    news_list = []\n",
        "    for entry in feed.entries[:5]:  # Fetch top 5 news articles\n",
        "        news_list.append({\"title\": entry.title, \"link\": entry.link})\n",
        "\n",
        "    return news_list\n",
        "\n",
        "news_data = get_google_news_rss(\"TCS\")\n",
        "for news in news_data:\n",
        "    print(\"\\n\",news[\"title\"], \"-\", news[\"link\"])\n",
        "\n",
        "def analyze_news_sentiment(news_data):\n",
        "    return TextBlob(news_data).sentiment.polarity\n",
        "\n",
        "def main():\n",
        "    stock_data = fetch_stock_data(STOCK_SYMBOL, ALPHA_VANTAGE_API_KEY)\n",
        "    stocktwits_data = fetch_stocktwits_data(STOCK_SYMBOL)\n",
        "\n",
        "    if stocktwits_data and 'messages' in stocktwits_data:\n",
        "        sentiment_analysis = analyze_sentiment(stocktwits_data['messages'])\n",
        "        df_sentiment = pd.DataFrame(sentiment_analysis)\n",
        "        df_sentiment.to_csv(\"sentiment_data.csv\", index=False)\n",
        "        print(\"Sentiment data saved successfully.\")\n",
        "    else:\n",
        "        print(\"No messages found in StockTwits data.\")\n",
        "    news_text_sample = \"Stock markets rally as tech stocks soar.\"\n",
        "    print(\"Sample News Sentiment Score:\", analyze_news_sentiment(news_text_sample))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9meFyDis_mnP",
        "outputId": "f7d6ea49-327e-404a-8431-eb620acd0bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " TCS, Infosys, HDFC Bank, HCL Tech among 5 key stocks to declare interim dividend in April 2025 - Mint - https://news.google.com/rss/articles/CBMi8wFBVV95cUxQOXJ0TEd1UWVmTXRSS0UxYThjNnJjaS1SWWJReGdLS1RYMVAwR2pwaE00OExTQzhKeFdUckVYTlh3dFNZQ19vQkV2TG5zdVRqd19ZVXVaRlF4TFk4MGZBR2EtV3YzZXI5RkNzU01icHZCbVo0YTdhemstVU1EN2wwaUdUYnFINk5DRTlsbk95UTJfeFQ0WUYyVkxFRFZKaG5Cc212QVFycjJxVFZVRkZuN09oQjBPRF9fTmc5RzlrZG1tMW1HMjJuUHpHWWJSU3RtT2NsOWpPbkJtQ3hxMUJWRTA5c0lnLTE3WXhGS21tXzQ3OUXSAfgBQVVfeXFMTXd6aFRBUEROLW95SDdmeVU0dldqREJxdGpIOUJYV2x4XzFYS21zNlNZa3JmR1o5ZXpYVEQxSVB3NVVBblRWb0haenFVSGJqdHFONzllUlFXcVhINlplN2Vicm5kQS1oZlZIbFo5ZHltNVVscDBIdXl0Nm5EY1Zud3UtRmZMQmF4SHZlaWo4NGdCVEFDU1RyNlUtcDE2cGxTdGdfR01TdGFrTG8wVU93TlhLTktFNXVHZE50ejN0VUNTNnNxRzh4RmlwejJQNkdoTmNFUWtYZTYtcmR3cGdhazdibnJ6a0pmS3pyR0s1TGU3WHhiZTE3Tjg?oc=5\n",
            "\n",
            " 32% target price slash! Goldman flags big risks for TCS, Infosys & other IT stocks amid US worries - The Economic Times - https://news.google.com/rss/articles/CBMi3AFBVV95cUxPdVdENUlnZEhJb213YkxKX2R0RE51Sm9FRGlRbGhwQlNXR3dfNVRpeGZNVHZ3b09CWGhEak9hdmlyOTFzekZ6dGdORVhhNWZIR3FjOG9FTkNoR0ZfNWxVS2RQQnhRYjgzSUZPZGdRTjc2bUdhUG1SWWc3OVdOVF9VRjNZeURCWjNSQlo4LVRKcVVYb0lGVmpRR1NkUFJEMThQR1dvUzVIZDNFczQ4WmFiNzhLbXdUT0FrUWVkNmpDbl9CRWlzaC1vTmk1M2huQnRvWDVIREsxMU8tOFU50gHiAUFVX3lxTFBteEZETU1iQkNMUUZoSzZlUURqaTdVNEVCVlEyZmdLdUZwVmpDS2FrSjEyZ1ZJSzhxVldETVpzYjVGbFhaZGVXVTdXazZzZHY5c3VYSUM4U2hGejZHQ0pKNFRkTjB4R1hvdkRnbFlpZjhkLVl0U3NBZVpxeS1fMU1hTUpTU190Ym5YaTZ6UzBCQXUwNE0zbWV0VkJ3cXlNRTZuVzA1R0VueUJqTWo5SHJVZFJkcXhUWWpQOXRKWXpud29JU1BEQkx4VFRjd1YzbWUzNEMwWk9KbU5LcjgxYTdkcHc?oc=5\n",
            "\n",
            " LTIMindtree shares downgraded by Goldman Sachs, price targets cut for TCS, Infosys - CNBCTV18 - https://news.google.com/rss/articles/CBMi0AFBVV95cUxPaFFTTmNJN0VsQVJBZENhTFRuMjcwZWtsUGswc0dMRTQtbmVCTDVWMFBzRlNXSHN6Q0t6SzR4N1pCeUFkRmZERjFsOWJBckN4UUhsaXU2bWd4R0dzSFN2LUt2UjB3amhTR25uOVA4UndRbmhhNndtMDRfblZaTEdVQUZMaUdqQ2pkTk5VcDEzOGtmdmRYZ2xSTUFkakZBbWx0aUpsbGo0blBEYXdEU3hrR3FfSlBDZHNNZ2RCSC14RWFvSlNWM0REcjA5QnEzWEVs0gHWAUFVX3lxTFBQdHFrZ3M5ZC14ajZkSkJnOTZGNnprNWhvQ09zcDFqUmpHS1pNLWk0TVYwOGFLM0hvenBUSnpLSFRoWHdMcHZhc3ZnVEhGXzJENkJfMXVQdDhZcnhUb1k5UzNqTk1YMTREOEprTDdfTkxxa2xBZE04S2FkRjZIdktlQUNCdngyQlljOWZJNzRQR3FxWXdYU1lhQ1pOazRJSnhBRjllMWN0T0tUbmlLTENTWDlkNkc2RWtOem14Ull3bVhKb0FtOGFPR2tsalZBcFp0NkxBNFE?oc=5\n",
            "\n",
            " IT Stocks, Wipro, Infosys, TCS Share Price Highlights: Wipro ends nearly 2% higher, Infosys, TCS, HCL Tech shares end flat - BusinessLine - https://news.google.com/rss/articles/CBMi2gFBVV95cUxOcldqU0kzVHhmVkRQRVhXVzhiMHA2WDRZM3htb2hxQVpqWmxFVGp1MTV6OWlIZFg3YnFwUmRlY2RnM1BnLVJvaVdUS2FCXzlHOHk3QzdubW1sS0JxeVhtcFVmN3Rqb3RNalVHRHVzaFZ2eGp0RGdKTUlJTzUzTFBOaHFkaWwzN1J5LVdoMXgyWmZOS09zMVd0MmxzZExUQUJPOHdiZ2tSMnF3R2tXWURzaEFKNVpjd21jaEI3R2daU0gzZmI1S0tva3RjV2xYUjZyd2xNZTZhS0o3dw?oc=5\n",
            "\n",
            " TCS, Infosys, HDFC Bank, HCL Tech among 5 key stocks to declare interim dividend in April 2025 - MSN - https://news.google.com/rss/articles/CBMi2wFBVV95cUxQUzN6YlQtdEJ6WDJUTFN5aFdCaFlxdGxSeXRoZzZvOFlfU3cyZjJITGUzOG5aRV9WXzBWd2tDNmZrbi1tWDg2YjNzZGRGNy1wczl2eDRnNG5qZV9Tb3V1b0dtWEZGdzUtUVhQTERGTC1KU2tUbGhpNGF0cEd2UURCcFZkdWhyNVhRN1lPYTFNVHpZSUJIbWJBQTJycWhNV0lOQTVfLWI4bVB1MzJjeDdDMFozTkd2NmFybEd6ZmpEc3lSUUN5M0Z2XzN0X0VFekt2dHY3dElHbnFEdzQ?oc=5\n",
            "\n",
            "Stock data for AAPL saved successfully.\n",
            "Successfully fetched StockTwits data for AAPL\n",
            "Sentiment data saved successfully.\n",
            "Sample News Sentiment Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# April Monthly Progress"
      ],
      "metadata": {
        "id": "2WKoLnHCfdAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy requests matplotlib seaborn scikit-learn xgboost nltk tweepy newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSBOEEPBfi75",
        "outputId": "24c7f2d2-aa71-4c32-bfad-1866918a29e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.2)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=2f2c146c728cf86b78a09435c3c00faee90c21be1bf7902a9fe1327e1bd19322\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=fad6c91694161d0236fb4d7ae9e9bd7d53935964662cefd27cabe881a13a122d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=eb0d6a9f468a11563a323e0b73deb1dbd236351cd1c2f6de23c6039981a3b8a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=47ec1b7ee4c9f148c5390ae9678069f0dd12d0ea74f3c068dc0ab9eada3caed3\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emruFvD6gcQX",
        "outputId": "d1afb880-7e4b-4b39-f688-0864e71141b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlZt9Cx4hmbU",
        "outputId": "49831e43-1dc3-4ac9-bbc8-ba2877b19ad0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.2)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import tweepy\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# installing sentiment analyzer model\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "# initializing this instance\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Historical Stock Data Setup\n",
        "ALPHA_VANTAGE_API_KEY = \"PIG3WPABVKTBMH6Y\"\n",
        "\n",
        "# Twitter API setup\n",
        "TWITTER_API_KEY = \"ZkBtakhypMnFkI4dUzVo0QJTw\"\n",
        "TWITTER_API_SECRET = \"P1gqWUJsOFkjmkOsPuyB458xi8bwo4KZ1Cy0LbGXPQLxcR3v79\"\n",
        "TWITTER_ACCESS_TOKEN = \"1916142259252432899-t6yhBBktXrrexsMqb0DOR9ZgVDJJTB\"\n",
        "TWITTER_ACCESS_SECRET = \"L29j1XSBBYhJ614Ev2cE7Ukl3a0Vs5iwMxC9iyOEOJ3oR\"\n",
        "\n",
        "# Authenticate with Twitter API\n",
        "auth = tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_API_SECRET)\n",
        "auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_SECRET)\n",
        "twitter_api = tweepy.API(auth)\n",
        "\n",
        "# Class for stock manipulation detection\n",
        "class StockManipulationDetector:\n",
        "    def __init__(self, ticker_symbol, lookback_days=30):\n",
        "        self.ticker = ticker_symbol\n",
        "        self.lookback_days = lookback_days\n",
        "        self.stock_data = None\n",
        "        self.tweets = None\n",
        "        self.news = None\n",
        "        self.anomaly_model = None\n",
        "        self.manipulation_model = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def fetch_stock_data(self):\n",
        "        \"\"\"Fetch historical stock data from Alpha Vantage\"\"\"\n",
        "        print(f\"Fetching stock data for {self.ticker}...\")\n",
        "\n",
        "        # Daily data\n",
        "        url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={self.ticker}&outputsize=full&apikey={ALPHA_VANTAGE_API_KEY}\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        if \"Time Series (Daily)\" not in data:\n",
        "            print(\"Error fetching stock data. API response:\", data)\n",
        "            return False\n",
        "\n",
        "        df = pd.DataFrame(data[\"Time Series (Daily)\"]).T\n",
        "        df.columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "        df = df.astype(float)\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        df = df.sort_index()\n",
        "\n",
        "        # Calculate additional features\n",
        "        df['price_change'] = df['close'].pct_change()\n",
        "        df['volume_change'] = df['volume'].pct_change()\n",
        "        df['high_low_diff'] = df['high'] - df['low']\n",
        "        df['volatility'] = df['price_change'].rolling(window=5).std()\n",
        "\n",
        "        # Rolling statistics\n",
        "        df['price_mean_5d'] = df['close'].rolling(window=5).mean()\n",
        "        df['volume_mean_5d'] = df['volume'].rolling(window=5).mean()\n",
        "        df['price_std_5d'] = df['close'].rolling(window=5).std()\n",
        "        df['volume_std_5d'] = df['volume'].rolling(window=5).std()\n",
        "\n",
        "        # Z-scores for anomaly detection\n",
        "        df['price_z_score'] = (df['close'] - df['price_mean_5d']) / df['price_std_5d']\n",
        "        df['volume_z_score'] = (df['volume'] - df['volume_mean_5d']) / df['volume_std_5d']\n",
        "\n",
        "        # Momentum indicators\n",
        "        df['price_momentum'] = df['close'] - df['close'].shift(5)\n",
        "        df['volume_momentum'] = df['volume'] - df['volume'].shift(5)\n",
        "\n",
        "        # Filter to relevant period and drop NAs\n",
        "        df = df.iloc[-self.lookback_days*2:]\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        self.stock_data = df\n",
        "        print(f\"Fetched {len(df)} days of stock data\")\n",
        "        return True\n",
        "\n",
        "    # Simplified Twitter sentiment collection using text files or CSV instead of API\n",
        "    def fetch_tweets_alternative(self, ticker):\n",
        "        \"\"\"Alternative method when Twitter API is unavailable\"\"\"\n",
        "        print(f\"Using alternative sentiment data for {ticker}...\")\n",
        "\n",
        "        # Create synthetic sentiment data based on stock price movements\n",
        "        # This is a fallback when real Twitter data isn't available\n",
        "        if self.stock_data is not None:\n",
        "            dates = self.stock_data.index[-30:]  # Last 30 days\n",
        "\n",
        "            # Create synthetic tweet sentiment that somewhat follows price changes\n",
        "            # but with some randomness and lag\n",
        "            price_changes = self.stock_data['price_change'].values[-32:-2]  # Lagged by 2 days\n",
        "\n",
        "            synthetic_tweet_data = []\n",
        "\n",
        "            for i, date in enumerate(dates):\n",
        "                # Base sentiment on lagged price changes with noise\n",
        "                base_sentiment = price_changes[i] * 5  # Scale up for sentiment range\n",
        "                sentiment = min(max(base_sentiment + np.random.normal(0, 0.3), -1), 1)  # Bound between -1 and 1\n",
        "\n",
        "                # Create more tweets on volatile days\n",
        "                tweet_count = int(50 + abs(sentiment) * 200 + np.random.normal(0, 20))\n",
        "                tweet_count = max(10, tweet_count)  # At least 10 tweets\n",
        "\n",
        "                synthetic_tweet_data.append({\n",
        "                    'date': date,\n",
        "                    'tweet_sentiment_mean': sentiment,\n",
        "                    'tweet_sentiment_std': 0.3 + abs(sentiment) * 0.2,\n",
        "                    'tweet_count': tweet_count,\n",
        "                    'retweet_count': tweet_count * 3,\n",
        "                    'favorite_count': tweet_count * 5\n",
        "                })\n",
        "\n",
        "            self.tweets = pd.DataFrame(synthetic_tweet_data)\n",
        "            self.tweets.set_index('date', inplace=True)\n",
        "            print(f\"Created synthetic sentiment data for {ticker}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    # Simplified news collection using text files or CSV instead of web scraping\n",
        "    def fetch_news_alternative(self, ticker):\n",
        "        \"\"\"Alternative method when news scraping is blocked\"\"\"\n",
        "        print(f\"Using alternative news data for {ticker}...\")\n",
        "\n",
        "        # Create synthetic news data based on stock movements\n",
        "        if self.stock_data is not None:\n",
        "            dates = self.stock_data.index[-30:]  # Last 30 days\n",
        "\n",
        "            # Create news sentiment that somewhat follows price trends\n",
        "            # but with occasional contrarian articles\n",
        "            synthetic_news_data = []\n",
        "\n",
        "            for date in dates:\n",
        "                # Get price data for this date if available\n",
        "                if date in self.stock_data.index:\n",
        "                    price_change = self.stock_data.loc[date, 'price_change']\n",
        "\n",
        "                    # Occasionally have contrarian news\n",
        "                    contrarian = np.random.random() > 0.7\n",
        "\n",
        "                    if contrarian:\n",
        "                        # News sentiment opposite to price movement\n",
        "                        sentiment = -price_change * 3\n",
        "                    else:\n",
        "                        # News sentiment aligned with price movement\n",
        "                        sentiment = price_change * 3\n",
        "\n",
        "                    sentiment = min(max(sentiment + np.random.normal(0, 0.2), -1), 1)\n",
        "\n",
        "                    # More news on days with bigger price moves\n",
        "                    news_count = int(2 + abs(price_change) * 20 + np.random.normal(0, 1))\n",
        "                    news_count = max(1, news_count)  # At least 1 news item\n",
        "\n",
        "                    synthetic_news_data.append({\n",
        "                        'date': date,\n",
        "                        'news_sentiment_mean': sentiment,\n",
        "                        'news_sentiment_std': 0.2,\n",
        "                        'news_count': news_count\n",
        "                    })\n",
        "\n",
        "            self.news = pd.DataFrame(synthetic_news_data)\n",
        "            self.news.set_index('date', inplace=True)\n",
        "            print(f\"Created synthetic news data for {ticker}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def analyze_sentiment(self):\n",
        "        \"\"\"Analyze sentiment from tweets and news, aggregate by day\"\"\"\n",
        "        print(\"Analyzing sentiment data...\")\n",
        "\n",
        "        # Create date ranges for the period we're analyzing\n",
        "        end_date = datetime.now().date()\n",
        "        start_date = end_date - timedelta(days=self.lookback_days)\n",
        "        date_range = pd.date_range(start=start_date, end=end_date)\n",
        "\n",
        "        # Initialize sentiment DataFrames\n",
        "        sentiment_daily = pd.DataFrame(index=date_range)\n",
        "\n",
        "        # Process tweets sentiment\n",
        "        if self.tweets is not None and not self.tweets.empty:\n",
        "            # Convert to datetime and extract date\n",
        "            self.tweets['date'] = self.tweets['created_at'].dt.date\n",
        "\n",
        "            # Group by date and calculate metrics\n",
        "            tweet_sentiment = self.tweets.groupby('date').agg({\n",
        "                'sentiment': ['mean', 'std', 'count'],\n",
        "                'retweet_count': 'sum',\n",
        "                'favorite_count': 'sum'\n",
        "            })\n",
        "\n",
        "            tweet_sentiment.columns = ['tweet_sentiment_mean', 'tweet_sentiment_std',\n",
        "                                     'tweet_count', 'retweet_count', 'favorite_count']\n",
        "\n",
        "            # Convert index to datetime for proper joining\n",
        "            tweet_sentiment.index = pd.to_datetime(tweet_sentiment.index)\n",
        "\n",
        "            # Join with main sentiment DataFrame\n",
        "            sentiment_daily = sentiment_daily.join(tweet_sentiment)\n",
        "\n",
        "        # Process news sentiment\n",
        "        if self.news is not None and not self.news.empty:\n",
        "            # Convert to datetime and extract date\n",
        "            self.news['date'] = self.news['published_date'].dt.date\n",
        "\n",
        "            # Group by date and calculate metrics\n",
        "            news_sentiment = self.news.groupby('date').agg({\n",
        "                'sentiment': ['mean', 'std', 'count']\n",
        "            })\n",
        "\n",
        "            news_sentiment.columns = ['news_sentiment_mean', 'news_sentiment_std', 'news_count']\n",
        "\n",
        "            # Convert index to datetime for proper joining\n",
        "            news_sentiment.index = pd.to_datetime(news_sentiment.index)\n",
        "\n",
        "            # Join with main sentiment DataFrame\n",
        "            sentiment_daily = sentiment_daily.join(news_sentiment)\n",
        "\n",
        "        # Fill NaN values with 0 for calculation purposes\n",
        "        sentiment_daily = sentiment_daily.fillna(0)\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        if 'tweet_sentiment_mean' in sentiment_daily.columns:\n",
        "            sentiment_daily['tweet_sentiment_zscore'] = (\n",
        "                sentiment_daily['tweet_sentiment_mean'] -\n",
        "                sentiment_daily['tweet_sentiment_mean'].rolling(window=5).mean()\n",
        "            ) / sentiment_daily['tweet_sentiment_mean'].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        if 'news_sentiment_mean' in sentiment_daily.columns:\n",
        "            sentiment_daily['news_sentiment_zscore'] = (\n",
        "                sentiment_daily['news_sentiment_mean'] -\n",
        "                sentiment_daily['news_sentiment_mean'].rolling(window=5).mean()\n",
        "            ) / sentiment_daily['news_sentiment_mean'].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        # Add sentiment momentum (change from previous day)\n",
        "        for col in ['tweet_sentiment_mean', 'news_sentiment_mean']:\n",
        "            if col in sentiment_daily.columns:\n",
        "                sentiment_daily[f'{col}_change'] = sentiment_daily[col].diff()\n",
        "\n",
        "        # Add volume change metrics\n",
        "        for col in ['tweet_count', 'news_count']:\n",
        "            if col in sentiment_daily.columns:\n",
        "                sentiment_daily[f'{col}_change'] = sentiment_daily[col].pct_change()\n",
        "                sentiment_daily[f'{col}_zscore'] = (\n",
        "                    sentiment_daily[col] -\n",
        "                    sentiment_daily[col].rolling(window=5).mean()\n",
        "                ) / sentiment_daily[col].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        return sentiment_daily\n",
        "\n",
        "    def integrate_data(self):\n",
        "        \"\"\"Integrate stock data with sentiment analysis\"\"\"\n",
        "        print(\"Integrating market and sentiment data...\")\n",
        "\n",
        "        # Get sentiment data\n",
        "        sentiment_daily = self.analyze_sentiment()\n",
        "\n",
        "        # Make sure stock_data index is datetime\n",
        "        self.stock_data.index = pd.to_datetime(self.stock_data.index)\n",
        "\n",
        "        # Merge sentiment with stock data\n",
        "        merged_data = self.stock_data.join(sentiment_daily, how='left')\n",
        "\n",
        "        # Fill missing values\n",
        "        merged_data = merged_data.fillna(0)\n",
        "\n",
        "        # Calculate correlations between sentiment and price/volume changes\n",
        "        # These correlations can help identify manipulation\n",
        "        if 'tweet_sentiment_mean' in merged_data.columns:\n",
        "            merged_data['tweet_price_corr'] = merged_data['tweet_sentiment_mean'].rolling(window=5).corr(merged_data['price_change'])\n",
        "            merged_data['tweet_volume_corr'] = merged_data['tweet_sentiment_mean'].rolling(window=5).corr(merged_data['volume_change'])\n",
        "\n",
        "        if 'news_sentiment_mean' in merged_data.columns:\n",
        "            merged_data['news_price_corr'] = merged_data['news_sentiment_mean'].rolling(window=5).corr(merged_data['price_change'])\n",
        "            merged_data['news_volume_corr'] = merged_data['news_sentiment_mean'].rolling(window=5).corr(merged_data['volume_change'])\n",
        "\n",
        "        # Add features that might indicate manipulation\n",
        "        # 1. Abnormal price changes with high sentiment but low news (pump)\n",
        "        if 'tweet_count' in merged_data.columns and 'news_count' in merged_data.columns:\n",
        "            merged_data['pump_indicator'] = (\n",
        "                (merged_data['price_z_score'] > 1.5) &\n",
        "                (merged_data['tweet_sentiment_zscore'] > 1.5) &\n",
        "                (merged_data['news_count'] < merged_data['news_count'].mean())\n",
        "            ).astype(int)\n",
        "\n",
        "        # 2. High volume with negative sentiment divergence (dump)\n",
        "        if 'tweet_sentiment_zscore' in merged_data.columns:\n",
        "            merged_data['dump_indicator'] = (\n",
        "                (merged_data['volume_z_score'] > 1.5) &\n",
        "                (merged_data['price_change'] < 0) &\n",
        "                (merged_data['tweet_sentiment_zscore'] < -1.5)\n",
        "            ).astype(int)\n",
        "\n",
        "        # Keep only the most recent lookback days\n",
        "        merged_data = merged_data.iloc[-self.lookback_days:]\n",
        "\n",
        "        return merged_data\n",
        "\n",
        "    def train_anomaly_model(self, data):\n",
        "        \"\"\"Train isolation forest model for anomaly detection\"\"\"\n",
        "        print(\"Training anomaly detection model...\")\n",
        "\n",
        "        # Select features for anomaly detection\n",
        "        feature_cols = [\n",
        "            'price_z_score', 'volume_z_score', 'volatility',\n",
        "            'high_low_diff', 'price_momentum', 'volume_momentum'\n",
        "        ]\n",
        "\n",
        "        # Add sentiment features if available\n",
        "        sentiment_features = [\n",
        "            'tweet_sentiment_zscore', 'news_sentiment_zscore',\n",
        "            'tweet_count_zscore', 'news_count_zscore',\n",
        "            'tweet_price_corr', 'news_price_corr'\n",
        "        ]\n",
        "\n",
        "        for feature in sentiment_features:\n",
        "            if feature in data.columns:\n",
        "                feature_cols.append(feature)\n",
        "\n",
        "        # Get feature subset that exists in the data\n",
        "        valid_features = [col for col in feature_cols if col in data.columns]\n",
        "\n",
        "        if not valid_features:\n",
        "            print(\"No valid features found for anomaly detection\")\n",
        "            return False\n",
        "\n",
        "        # Extract features\n",
        "        X = data[valid_features].fillna(0)\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Train isolation forest\n",
        "        self.anomaly_model = IsolationForest(\n",
        "            n_estimators=100,\n",
        "            contamination=0.05,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        self.anomaly_model.fit(X_scaled)\n",
        "\n",
        "        # Add anomaly scores to the data\n",
        "        data['anomaly_score'] = self.anomaly_model.decision_function(X_scaled)\n",
        "        data['is_anomaly'] = self.anomaly_model.predict(X_scaled)\n",
        "\n",
        "        # Convert prediction to binary (1 for normal, -1 for anomaly)\n",
        "        data['is_anomaly'] = (data['is_anomaly'] == -1).astype(int)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def train_manipulation_model(self, data):\n",
        "        \"\"\"Train XGBoost model to classify potential manipulation\"\"\"\n",
        "        print(\"Training manipulation detection model...\")\n",
        "\n",
        "        # For a real system, you would have labeled data of known manipulation cases\n",
        "        # Since we don't have labels, we'll create synthetic ones based on our indicators\n",
        "\n",
        "        # Define manipulation as days with anomalies and either pump or dump indicators\n",
        "        if 'pump_indicator' in data.columns and 'dump_indicator' in data.columns:\n",
        "            data['potential_manipulation'] = (\n",
        "                (data['is_anomaly'] == 1) &\n",
        "                ((data['pump_indicator'] == 1) | (data['dump_indicator'] == 1))\n",
        "            ).astype(int)\n",
        "        else:\n",
        "            # Fallback to just anomalies if we don't have the indicators\n",
        "            data['potential_manipulation'] = data['is_anomaly']\n",
        "\n",
        "        # Select features for the classifier\n",
        "        feature_cols = [\n",
        "            'price_z_score', 'volume_z_score', 'volatility',\n",
        "            'high_low_diff', 'price_momentum', 'volume_momentum'\n",
        "        ]\n",
        "\n",
        "        # Add sentiment features if available\n",
        "        sentiment_features = [\n",
        "            'tweet_sentiment_mean', 'news_sentiment_mean',\n",
        "            'tweet_count', 'news_count',\n",
        "            'tweet_sentiment_zscore', 'news_sentiment_zscore',\n",
        "            'tweet_price_corr', 'news_price_corr'\n",
        "        ]\n",
        "\n",
        "        for feature in sentiment_features:\n",
        "            if feature in data.columns:\n",
        "                feature_cols.append(feature)\n",
        "\n",
        "        # Get feature subset that exists in the data\n",
        "        valid_features = [col for col in feature_cols if col in data.columns]\n",
        "\n",
        "        if not valid_features:\n",
        "            print(\"No valid features found for manipulation model\")\n",
        "            return data\n",
        "\n",
        "        # Extract features and target\n",
        "        X = data[valid_features].fillna(0)\n",
        "        y = data['potential_manipulation']\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Train XGBoost model\n",
        "        self.manipulation_model = xgb.XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=3,\n",
        "            random_state=42,\n",
        "            use_label_encoder=False,\n",
        "            eval_metric='logloss'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Only train if we have both classes represented\n",
        "            if len(y.unique()) > 1:\n",
        "                self.manipulation_model.fit(X_scaled, y)\n",
        "\n",
        "                # Add predictions to the data\n",
        "                data['manipulation_probability'] = self.manipulation_model.predict_proba(X_scaled)[:, 1]\n",
        "                data['predicted_manipulation'] = self.manipulation_model.predict(X_scaled)\n",
        "\n",
        "                # Feature importance\n",
        "                feature_importance = pd.DataFrame({\n",
        "                    'feature': valid_features,\n",
        "                    'importance': self.manipulation_model.feature_importances_\n",
        "                }).sort_values('importance', ascending=False)\n",
        "\n",
        "                print(\"\\nTop manipulation indicators:\")\n",
        "                print(feature_importance.head(5))\n",
        "            else:\n",
        "                print(\"Not enough variation in the target variable to train classifier\")\n",
        "                data['manipulation_probability'] = 0\n",
        "                data['predicted_manipulation'] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training manipulation model: {e}\")\n",
        "            data['manipulation_probability'] = 0\n",
        "            data['predicted_manipulation'] = 0\n",
        "\n",
        "        return data\n",
        "\n",
        "    def detect_manipulation(self, demo_mode=True):\n",
        "        \"\"\"Main method to run the entire detection pipeline\"\"\"\n",
        "        # Fetch data\n",
        "        if not self.fetch_stock_data():\n",
        "            print(\"Failed to fetch stock data. Aborting.\")\n",
        "            return None\n",
        "\n",
        "        if demo_mode:\n",
        "            # Use alternative data sources that don't require APIs\n",
        "            self.fetch_tweets_alternative(self.ticker)\n",
        "            self.fetch_news_alternative(self.ticker)\n",
        "        else:\n",
        "            # Try to use actual APIs (may fail with current limitations)\n",
        "            self.fetch_tweets()\n",
        "            self.fetch_news()\n",
        "\n",
        "    # Rest of the method remains the same...\n",
        "\n",
        "    def display_results(self, data):\n",
        "        \"\"\"Display detection results and visualizations\"\"\"\n",
        "        if data is None or data.empty:\n",
        "            print(\"No data available to display results\")\n",
        "            return\n",
        "\n",
        "        # Print summary of detected manipulations\n",
        "        print(\"\\n----- MANIPULATION DETECTION SUMMARY -----\")\n",
        "\n",
        "        # Filter to just the most recent period\n",
        "        recent_data = data.iloc[-self.lookback_days:]\n",
        "\n",
        "        # Count days with potential manipulation\n",
        "        if 'predicted_manipulation' in recent_data.columns:\n",
        "            manipulation_days = recent_data[recent_data['predicted_manipulation'] == 1]\n",
        "            n_manipulation_days = len(manipulation_days)\n",
        "\n",
        "            print(f\"Detected potential manipulation on {n_manipulation_days} days out of {len(recent_data)} analyzed.\")\n",
        "\n",
        "            if n_manipulation_days > 0:\n",
        "                print(\"\\nDates with suspected manipulation:\")\n",
        "                for date, row in manipulation_days.iterrows():\n",
        "                    features = []\n",
        "\n",
        "                    # Add indicators that triggered the alert\n",
        "                    if row['price_z_score'] > 1.5:\n",
        "                        features.append(f\"Abnormal price (z={row['price_z_score']:.2f})\")\n",
        "                    if row['volume_z_score'] > 1.5:\n",
        "                        features.append(f\"Abnormal volume (z={row['volume_z_score']:.2f})\")\n",
        "                    if 'tweet_sentiment_zscore' in row and row['tweet_sentiment_zscore'] > 1.5:\n",
        "                        features.append(f\"Abnormal social sentiment (z={row['tweet_sentiment_zscore']:.2f})\")\n",
        "                    if 'pump_indicator' in row and row['pump_indicator'] == 1:\n",
        "                        features.append(\"Pump pattern\")\n",
        "                    if 'dump_indicator' in row and row['dump_indicator'] == 1:\n",
        "                        features.append(\"Dump pattern\")\n",
        "\n",
        "                    print(f\"  {date.date()}: {', '.join(features)}\")\n",
        "        else:\n",
        "            print(\"Manipulation classification not available.\")\n",
        "\n",
        "        # Plot results\n",
        "        try:\n",
        "            self.plot_results(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating plots: {e}\")\n",
        "\n",
        "    def plot_results(self, data):\n",
        "        \"\"\"Create visualizations of the detection results\"\"\"\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(14, 18), sharex=True)\n",
        "\n",
        "        # Stock price with anomaly highlighting\n",
        "        ax0 = axes[0]\n",
        "        ax0.set_title(f\"{self.ticker} Stock Price with Anomaly Detection\", fontsize=14)\n",
        "        ax0.plot(data.index, data['close'], label='Close Price', color='blue')\n",
        "\n",
        "        # Highlight anomalies if available\n",
        "        if 'is_anomaly' in data.columns:\n",
        "            anomaly_days = data[data['is_anomaly'] == 1]\n",
        "            ax0.scatter(anomaly_days.index, anomaly_days['close'],\n",
        "                      color='red', label='Anomalies', zorder=5)\n",
        "\n",
        "        # Highlight manipulation if available\n",
        "        if 'predicted_manipulation' in data.columns:\n",
        "            manip_days = data[data['predicted_manipulation'] == 1]\n",
        "            ax0.scatter(manip_days.index, manip_days['close'],\n",
        "                      color='darkred', marker='X', s=100,\n",
        "                      label='Potential Manipulation', zorder=10)\n",
        "\n",
        "        ax0.set_ylabel('Price ($)')\n",
        "        ax0.legend()\n",
        "        ax0.grid(True, alpha=0.3)\n",
        "\n",
        "        # Volume plot\n",
        "        ax1 = axes[1]\n",
        "        ax1.set_title(f\"{self.ticker} Trading Volume\", fontsize=14)\n",
        "        ax1.bar(data.index, data['volume'], color='green', alpha=0.7, label='Volume')\n",
        "\n",
        "        # Highlight volume anomalies\n",
        "        volume_anomalies = data[data['volume_z_score'] > 1.5]\n",
        "        ax1.bar(volume_anomalies.index, volume_anomalies['volume'], color='orange', label='Volume Anomalies')\n",
        "\n",
        "        ax1.set_ylabel('Volume')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Sentiment plot\n",
        "        ax2 = axes[2]\n",
        "        ax2.set_title(\"Sentiment Analysis\", fontsize=14)\n",
        "\n",
        "        if 'tweet_sentiment_mean' in data.columns:\n",
        "            ax2.plot(data.index, data['tweet_sentiment_mean'],\n",
        "                   label='Social Sentiment', color='purple')\n",
        "\n",
        "        if 'news_sentiment_mean' in data.columns:\n",
        "            ax2.plot(data.index, data['news_sentiment_mean'],\n",
        "                   label='News Sentiment', color='brown')\n",
        "\n",
        "        # Add zero line\n",
        "        ax2.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
        "\n",
        "        ax2.set_ylabel('Sentiment Score')\n",
        "        ax2.set_xlabel('Date')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.ticker}_manipulation_analysis.png\")\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"\\nSaved visualization to {self.ticker}_manipulation_analysis.png\")\n",
        "\n",
        "def test_detector(ticker_symbol, demo_mode=True):\n",
        "    print(f\"\\n===== ANALYZING {ticker_symbol} =====\")\n",
        "    detector = StockManipulationDetector(ticker_symbol, lookback_days=30)\n",
        "    results = detector.detect_manipulation(demo_mode=demo_mode)\n",
        "    return results\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Test with a few stocks known for volatility and social media attention\n",
        "    tickers = [\"GME\", \"AMC\", \"TSLA\", \"AAPL\"]\n",
        "\n",
        "    for ticker in tickers:\n",
        "        test_detector(ticker)\n",
        "        # Add delay between API calls to avoid rate limits\n",
        "        time.sleep(2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Kmh-ucOLzaZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60f4f8c5-455b-4242-a959-42cc7d6940f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ANALYZING GME =====\n",
            "Fetching stock data for GME...\n",
            "Fetched 60 days of stock data\n",
            "Using alternative sentiment data for GME...\n",
            "Created synthetic sentiment data for GME\n",
            "Using alternative news data for GME...\n",
            "Created synthetic news data for GME\n",
            "\n",
            "===== ANALYZING AMC =====\n",
            "Fetching stock data for AMC...\n",
            "Fetched 60 days of stock data\n",
            "Using alternative sentiment data for AMC...\n",
            "Created synthetic sentiment data for AMC\n",
            "Using alternative news data for AMC...\n",
            "Created synthetic news data for AMC\n",
            "\n",
            "===== ANALYZING TSLA =====\n",
            "Fetching stock data for TSLA...\n",
            "Fetched 60 days of stock data\n",
            "Using alternative sentiment data for TSLA...\n",
            "Created synthetic sentiment data for TSLA\n",
            "Using alternative news data for TSLA...\n",
            "Created synthetic news data for TSLA\n",
            "\n",
            "===== ANALYZING AAPL =====\n",
            "Fetching stock data for AAPL...\n",
            "Fetched 60 days of stock data\n",
            "Using alternative sentiment data for AAPL...\n",
            "Created synthetic sentiment data for AAPL\n",
            "Using alternative news data for AAPL...\n",
            "Created synthetic news data for AAPL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING\n",
        "def create_test_dataset():\n",
        "    \"\"\"Create a dataset with known manipulation cases for testing\"\"\"\n",
        "    known_cases = [\n",
        "        {\"ticker\": \"GME\", \"start_date\": \"2021-01-22\", \"end_date\": \"2021-01-28\", \"type\": \"short_squeeze\"},\n",
        "        {\"ticker\": \"AMC\", \"start_date\": \"2021-01-25\", \"end_date\": \"2021-01-29\", \"type\": \"meme_stock\"},\n",
        "        {\"ticker\": \"HCMC\", \"start_date\": \"2021-02-01\", \"end_date\": \"2021-02-05\", \"type\": \"pump_and_dump\"},\n",
        "        # Add more known cases from SEC litigation releases or academic papers\n",
        "    ]\n",
        "    return known_cases\n",
        "\n",
        "def inject_manipulation_patterns(stock_data, pattern_type=\"pump_and_dump\"):\n",
        "    \"\"\"Inject synthetic manipulation patterns into real stock data\"\"\"\n",
        "    data = stock_data.copy()\n",
        "\n",
        "    if pattern_type == \"pump_and_dump\":\n",
        "        # Select a random 3-5 day window\n",
        "        window_size = np.random.randint(3, 6)\n",
        "        start_idx = np.random.randint(10, len(data) - window_size - 5)\n",
        "\n",
        "        # Create the pump phase (2-3 days)\n",
        "        pump_days = window_size - 1\n",
        "        for i in range(pump_days):\n",
        "            # Increase price by 5-15% each day\n",
        "            increase = 1 + (0.05 + 0.1 * np.random.random())\n",
        "            idx = start_idx + i\n",
        "\n",
        "            # Adjust OHLC prices\n",
        "            data.iloc[idx, data.columns.get_indexer(['open'])] *= increase\n",
        "            data.iloc[idx, data.columns.get_indexer(['high'])] *= increase * 1.02\n",
        "            data.iloc[idx, data.columns.get_indexer(['low'])] *= increase * 0.98\n",
        "            data.iloc[idx, data.columns.get_indexer(['close'])] *= increase\n",
        "\n",
        "            # Increase volume\n",
        "            data.iloc[idx, data.columns.get_indexer(['volume'])] *= (2 + i)\n",
        "\n",
        "        # Create the dump phase (1-2 days)\n",
        "        dump_idx = start_idx + pump_days\n",
        "        # Sharp decrease in price (20-30%)\n",
        "        data.iloc[dump_idx, data.columns.get_indexer(['open'])] *= 0.9\n",
        "        data.iloc[dump_idx, data.columns.get_indexer(['high'])] *= 0.95\n",
        "        data.iloc[dump_idx, data.columns.get_indexer(['low'])] *= 0.7\n",
        "        data.iloc[dump_idx, data.columns.get_indexer(['close'])] *= 0.75\n",
        "        # Huge volume on dump day\n",
        "        data.iloc[dump_idx, data.columns.get_indexer(['volume'])] *= 5\n",
        "\n",
        "        # Create \"ground truth\" labels\n",
        "        labels = pd.Series(0, index=data.index)\n",
        "        labels.iloc[start_idx:start_idx+window_size] = 1\n",
        "\n",
        "        return data, labels, start_idx, start_idx+window_size\n",
        "\n",
        "    # Add more pattern types as needed\n",
        "\n",
        "    return data, None, None, None\n",
        "\n",
        "def evaluate_model(detector, test_cases, verbose=True):\n",
        "    \"\"\"Evaluate the model against known manipulation cases\"\"\"\n",
        "    results = {\n",
        "        \"true_positives\": 0,\n",
        "        \"false_positives\": 0,\n",
        "        \"true_negatives\": 0,\n",
        "        \"false_negatives\": 0\n",
        "    }\n",
        "\n",
        "    case_results = []\n",
        "\n",
        "    for case in test_cases:\n",
        "        ticker = case[\"ticker\"]\n",
        "        start_date = pd.to_datetime(case[\"start_date\"])\n",
        "        end_date = pd.to_datetime(case[\"end_date\"])\n",
        "\n",
        "        # Run detector on this ticker\n",
        "        detector = StockManipulationDetector(ticker, lookback_days=60)\n",
        "        detection_results = detector.detect_manipulation(demo_mode=True)\n",
        "\n",
        "        if detection_results is None:\n",
        "            continue\n",
        "\n",
        "        # Check if manipulation was detected within the known manipulation period\n",
        "        if \"predicted_manipulation\" in detection_results.columns:\n",
        "            # Get dates in the manipulation period\n",
        "            manipulation_period = detection_results.loc[start_date:end_date]\n",
        "            detected_days = manipulation_period[manipulation_period[\"predicted_manipulation\"] == 1]\n",
        "\n",
        "            # Case was detected if at least one day in the period was flagged\n",
        "            case_detected = len(detected_days) > 0\n",
        "\n",
        "            # Calculate metrics\n",
        "            if case_detected:\n",
        "                results[\"true_positives\"] += 1\n",
        "            else:\n",
        "                results[\"false_negatives\"] += 1\n",
        "\n",
        "            # Check for false positives outside the manipulation period\n",
        "            false_positives = detection_results[\n",
        "                (detection_results.index < start_date) |\n",
        "                (detection_results.index > end_date)\n",
        "            ]\n",
        "            false_positives = false_positives[false_positives[\"predicted_manipulation\"] == 1]\n",
        "\n",
        "            results[\"false_positives\"] += len(false_positives)\n",
        "\n",
        "            # Store case-specific results\n",
        "            case_results.append({\n",
        "                \"ticker\": ticker,\n",
        "                \"manipulation_period\": f\"{start_date.date()} to {end_date.date()}\",\n",
        "                \"detected\": case_detected,\n",
        "                \"days_detected\": len(detected_days),\n",
        "                \"false_positives\": len(false_positives)\n",
        "            })\n",
        "\n",
        "    # Calculate metrics\n",
        "    if results[\"true_positives\"] + results[\"false_positives\"] > 0:\n",
        "        precision = results[\"true_positives\"] / (results[\"true_positives\"] + results[\"false_positives\"])\n",
        "    else:\n",
        "        precision = 0\n",
        "\n",
        "    if results[\"true_positives\"] + results[\"false_negatives\"] > 0:\n",
        "        recall = results[\"true_positives\"] / (results[\"true_positives\"] + results[\"false_negatives\"])\n",
        "    else:\n",
        "        recall = 0\n",
        "\n",
        "    if precision + recall > 0:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    else:\n",
        "        f1_score = 0\n",
        "\n",
        "    # Output results\n",
        "    if verbose:\n",
        "        print(\"\\n===== MODEL EVALUATION RESULTS =====\")\n",
        "        print(f\"True Positives: {results['true_positives']}\")\n",
        "        print(f\"False Positives: {results['false_positives']}\")\n",
        "        print(f\"False Negatives: {results['false_negatives']}\")\n",
        "        print(f\"Precision: {precision:.2f}\")\n",
        "        print(f\"Recall: {recall:.2f}\")\n",
        "        print(f\"F1 Score: {f1_score:.2f}\")\n",
        "\n",
        "        print(\"\\nDetailed Case Results:\")\n",
        "        for case in case_results:\n",
        "            status = \"✓ DETECTED\" if case[\"detected\"] else \"✗ MISSED\"\n",
        "            print(f\"{case['ticker']} ({case['manipulation_period']}): {status}\")\n",
        "            if case[\"detected\"]:\n",
        "                print(f\"  - Days detected: {case['days_detected']}\")\n",
        "            print(f\"  - False positives: {case['false_positives']}\")\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score,\n",
        "        \"case_results\": case_results,\n",
        "        \"raw_results\": results\n",
        "    }\n",
        "\n",
        "def run_model_test():\n",
        "    \"\"\"Run a comprehensive test of the model\"\"\"\n",
        "    # Get test cases\n",
        "    test_cases = create_test_dataset()\n",
        "\n",
        "    # Run evaluation\n",
        "    eval_results = evaluate_model(None, test_cases)\n",
        "\n",
        "    # Also test with synthetic data\n",
        "    print(\"\\n===== SYNTHETIC DATA TEST =====\")\n",
        "    for ticker in [\"MSFT\", \"GOOGL\", \"AMZN\"]:\n",
        "        # Create detector\n",
        "        detector = StockManipulationDetector(ticker, lookback_days=60)\n",
        "\n",
        "        # Get stock data\n",
        "        detector.fetch_stock_data()\n",
        "\n",
        "        # Inject manipulation pattern\n",
        "        modified_data, labels, start_idx, end_idx = inject_manipulation_patterns(detector.stock_data)\n",
        "\n",
        "        # Override stock data with manipulated version\n",
        "        detector.stock_data = modified_data\n",
        "\n",
        "        # Create synthetic sentiment and news that align with manipulation\n",
        "        detector.fetch_tweets_alternative(ticker)\n",
        "        detector.fetch_news_alternative(ticker)\n",
        "\n",
        "        # Modify sentiment data to align with manipulation\n",
        "        manipulation_dates = modified_data.index[start_idx:end_idx]\n",
        "        detector.tweets.loc[manipulation_dates, 'tweet_sentiment_mean'] = 0.8\n",
        "        detector.tweets.loc[manipulation_dates, 'tweet_count'] *= 3\n",
        "\n",
        "        # Run detection\n",
        "        results = detector.detect_manipulation(demo_mode=True)\n",
        "\n",
        "        # Evaluate against ground truth\n",
        "        detected_days = results.loc[manipulation_dates[0]:manipulation_dates[-1]]\n",
        "        detected_days = detected_days[detected_days['predicted_manipulation'] == 1]\n",
        "\n",
        "        print(f\"\\nTesting {ticker} with synthetic manipulation:\")\n",
        "        print(f\"Manipulation period: {manipulation_dates[0].date()} to {manipulation_dates[-1].date()}\")\n",
        "        print(f\"Days correctly identified: {len(detected_days)} out of {len(manipulation_dates)}\")\n",
        "\n",
        "        # Calculate precision/recall for this case\n",
        "        true_positives = len(detected_days)\n",
        "        false_negatives = len(manipulation_dates) - true_positives\n",
        "\n",
        "        false_positives = results[~results.index.isin(manipulation_dates)]\n",
        "        false_positives = false_positives[false_positives['predicted_manipulation'] == 1]\n",
        "\n",
        "        if true_positives + len(false_positives) > 0:\n",
        "            precision = true_positives / (true_positives + len(false_positives))\n",
        "        else:\n",
        "            precision = 0\n",
        "\n",
        "        if true_positives + false_negatives > 0:\n",
        "            recall = true_positives / (true_positives + false_negatives)\n",
        "        else:\n",
        "            recall = 0\n",
        "\n",
        "        print(f\"Precision: {precision:.2f}\")\n",
        "        print(f\"Recall: {recall:.2f}\")\n",
        "        if precision + recall > 0:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "            print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_model_test()\n",
        "\n",
        "def cross_validate_model(patterns=[\"pump_and_dump\", \"bear_raid\", \"wash_trading\"], tickers=[\"MSFT\", \"AAPL\", \"AMZN\", \"GOOGL\"]):\n",
        "    \"\"\"Cross-validate model on multiple manipulation patterns\"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    for pattern in patterns:\n",
        "        pattern_results = []\n",
        "\n",
        "        for ticker in tickers:\n",
        "            # Create detector\n",
        "            detector = StockManipulationDetector(ticker, lookback_days=60)\n",
        "\n",
        "            # Fetch stock data\n",
        "            detector.fetch_stock_data()\n",
        "\n",
        "            if detector.stock_data is None:\n",
        "                continue\n",
        "\n",
        "            # Inject manipulation pattern\n",
        "            modified_data, labels, start_idx, end_idx = inject_manipulation_patterns(\n",
        "                detector.stock_data, pattern_type=pattern\n",
        "            )\n",
        "\n",
        "            # Set up test\n",
        "            detector.stock_data = modified_data\n",
        "            detector.fetch_tweets_alternative(ticker)\n",
        "            detector.fetch_news_alternative(ticker)\n",
        "\n",
        "            # Run detection\n",
        "            results = detector.detect_manipulation(demo_mode=True)\n",
        "\n",
        "            # Evaluate against ground truth\n",
        "            manipulation_dates = modified_data.index[start_idx:end_idx]\n",
        "            accuracy_metrics = calculate_accuracy(results, manipulation_dates)\n",
        "\n",
        "            pattern_results.append({\n",
        "                \"ticker\": ticker,\n",
        "                \"pattern\": pattern,\n",
        "                **accuracy_metrics\n",
        "            })\n",
        "\n",
        "        all_results.extend(pattern_results)\n",
        "\n",
        "    # Calculate average metrics by pattern\n",
        "    pattern_summary = pd.DataFrame(all_results).groupby('pattern').mean()\n",
        "    print(\"\\n===== CROSS-VALIDATION RESULTS =====\")\n",
        "    print(pattern_summary[['precision', 'recall', 'f1_score']])\n",
        "\n",
        "    return all_results, pattern_summary\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "EI4RYRiRfac_",
        "outputId": "cec4dc79-71a2-4b91-e377-08adfccf5174"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching stock data for GME...\n",
            "Fetched 120 days of stock data\n",
            "Using alternative sentiment data for GME...\n",
            "Created synthetic sentiment data for GME\n",
            "Using alternative news data for GME...\n",
            "Created synthetic news data for GME\n",
            "Fetching stock data for AMC...\n",
            "Fetched 120 days of stock data\n",
            "Using alternative sentiment data for AMC...\n",
            "Created synthetic sentiment data for AMC\n",
            "Using alternative news data for AMC...\n",
            "Created synthetic news data for AMC\n",
            "Fetching stock data for HCMC...\n",
            "Fetched 120 days of stock data\n",
            "Using alternative sentiment data for HCMC...\n",
            "Created synthetic sentiment data for HCMC\n",
            "Using alternative news data for HCMC...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OverflowError",
          "evalue": "cannot convert float infinity to integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d10a99250413>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mrun_model_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcross_validate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pump_and_dump\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bear_raid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wash_trading\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MSFT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AAPL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AMZN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GOOGL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d10a99250413>\u001b[0m in \u001b[0;36mrun_model_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;31m# Also test with synthetic data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d10a99250413>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(detector, test_cases, verbose)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Run detector on this ticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStockManipulationDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookback_days\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mdetection_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_manipulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetection_results\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-7be9388795e0>\u001b[0m in \u001b[0;36mdetect_manipulation\u001b[0;34m(self, demo_mode)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;31m# Use alternative data sources that don't require APIs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_tweets_alternative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_news_alternative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;31m# Try to use actual APIs (may fail with current limitations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-7be9388795e0>\u001b[0m in \u001b[0;36mfetch_news_alternative\u001b[0;34m(self, ticker)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0;31m# More news on days with bigger price moves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mnews_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice_change\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                     \u001b[0mnews_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_count\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# At least 1 news item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fgGP0NI84M4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}