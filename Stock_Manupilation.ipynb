{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpGYdYAM47/TxnxbUC3tzP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvvvvvss/StockMarketManupilationSystem/blob/main/Stock_Manupilation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real-time data processing and analysis"
      ],
      "metadata": {
        "id": "VKCdRf2Jj3Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install aiohttp pandas confluent-kafka"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_40dDFqZjYIv",
        "outputId": "cc139140-0640-4c0a-981c-69bfb25e8d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.14)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: confluent-kafka in /usr/local/lib/python3.11/dist-packages (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.18.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to see what kind of data can be fetched from the API: Alphavantage"
      ],
      "metadata": {
        "id": "Kehg7vij0PE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "API_KEY = \"QT13WY791JO16QMJ\"\n",
        "BASE_URL = \"https://www.alphavantage.co/query\"\n",
        "\n",
        "def fetch_stock_data(symbol, interval=\"5min\"):\n",
        "    params = {\n",
        "        \"function\": \"TIME_SERIES_INTRADAY\",\n",
        "        \"symbol\": symbol,\n",
        "        \"interval\": interval,\n",
        "        \"apikey\": API_KEY,\n",
        "        \"outputsize\": \"compact\"\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"Error Message\" in data:\n",
        "        print(f\"Error fetching data for {symbol}: {data['Error Message']}\")\n",
        "        return None\n",
        "    elif f\"Time Series ({interval})\" in data:\n",
        "        time_series = data[f\"Time Series ({interval})\"]\n",
        "        df = pd.DataFrame.from_dict(time_series, orient=\"index\")\n",
        "        df.reset_index(inplace=True)\n",
        "        df.rename(columns={\"index\": \"timestamp\"}, inplace=True)\n",
        "        return df\n",
        "    else:\n",
        "        print(f\"Unexpected data format for {symbol}: {data}\")\n",
        "        return None\n",
        "\n",
        "stock_data = fetch_stock_data(\"AAPL\")\n",
        "if stock_data is not None:\n",
        "    print(stock_data.head())\n",
        "else:\n",
        "    print(\"Could not retrieve stock data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3muMVSN3PJ-",
        "outputId": "7b850295-0333-4042-ac88-769cfe39c548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             timestamp   1. open   2. high    3. low  4. close 5. volume\n",
            "0  2025-03-25 19:55:00  224.1300  224.3300  224.1000  224.2400      3316\n",
            "1  2025-03-25 19:50:00  224.2000  224.3300  224.0700  224.1000       738\n",
            "2  2025-03-25 19:45:00  224.1500  224.3300  224.0700  224.2800      2743\n",
            "3  2025-03-25 19:40:00  224.1500  224.1500  224.0700  224.0700       834\n",
            "4  2025-03-25 19:35:00  224.1000  224.1500  224.0500  224.0701       546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#    Data Collection - Rough\n",
        "\n",
        "1.   Fetch trading data from Alpha Vantage\n",
        "2.   Detect potential market manipulation using Isolation Forest\n",
        "3.   Mock implementation of social media sentiment collection\n",
        "\n"
      ],
      "metadata": {
        "id": "f3UoZNVs0owj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import requests\n",
        "\n",
        "class MarketManipulationDetector:\n",
        "    def __init__(self, alpha_vantage_key):\n",
        "        self.alpha_vantage_key = alpha_vantage_key\n",
        "        self.trading_data = None\n",
        "        self.sentiment_data = None\n",
        "\n",
        "    def fetch_trading_data(self, symbol):\n",
        "        url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&apikey={self.alpha_vantage_key}\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            raw_data = response.json()\n",
        "            self.trading_data = pd.DataFrame.from_dict(\n",
        "                raw_data.get('Time Series (Daily)', {}),\n",
        "                orient='index'\n",
        "            )\n",
        "            self.trading_data.columns = [\n",
        "                'open', 'high', 'low', 'close', 'volume'\n",
        "            ]\n",
        "            self.trading_data = self.trading_data.astype(float)\n",
        "\n",
        "    def detect_anomalous_trading(self):\n",
        "        if self.trading_data is None:\n",
        "            raise ValueError(\"Trading data not loaded\")\n",
        "\n",
        "\n",
        "        features = ['volume', 'close']\n",
        "        X = self.trading_data[features]\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        clf = IsolationForest(contamination=0.1, random_state=42)\n",
        "        y_pred = clf.fit_predict(X_scaled)\n",
        "        self.trading_data['is_anomaly'] = y_pred == -1\n",
        "\n",
        "        return self.trading_data[self.trading_data['is_anomaly']]\n",
        "\n",
        "    def collect_social_sentiment(self, symbol):\n",
        "        #  without StockTwits API\n",
        "\n",
        "        fake_sentiments = {\n",
        "            'bullish': 0.6,\n",
        "            'bearish': 0.3,\n",
        "            'neutral': 0.1\n",
        "        }\n",
        "        return fake_sentiments\n",
        "\n",
        "def main():\n",
        "\n",
        "    detector = MarketManipulationDetector(alpha_vantage_key='QT13WY791JO16QMJ')\n",
        "    detector.fetch_trading_data('INFY')\n",
        "\n",
        "    anomalies = detector.detect_anomalous_trading()\n",
        "    print(\"Potential Manipulative Trading Days:\")\n",
        "    print(anomalies)\n",
        "\n",
        "\n",
        "    sentiment = detector.collect_social_sentiment('INFY')\n",
        "    print(\"\\nSocial Media Sentiment:\")\n",
        "    print(sentiment)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDUXnDAq-ZR5",
        "outputId": "20aa68da-c4a7-4044-a4cb-19da435ccdde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Potential Manipulative Trading Days:\n",
            "             open    high      low  close      volume  is_anomaly\n",
            "2025-03-28  18.37  18.482  18.1050  18.17   7866062.0        True\n",
            "2025-03-27  18.70  18.780  18.5950  18.67   6249534.0        True\n",
            "2025-03-21  18.41  18.430  18.1700  18.32  18677618.0        True\n",
            "2025-03-20  18.33  18.390  17.9001  18.06  19376214.0        True\n",
            "2025-03-13  18.50  18.585  18.2600  18.29  10913566.0        True\n",
            "2025-03-12  18.49  18.645  18.3400  18.50  15292391.0        True\n",
            "2025-03-11  19.13  19.200  18.8100  18.97  17695135.0        True\n",
            "2025-01-16  22.60  22.600  21.3100  21.57  22922717.0        True\n",
            "2024-12-19  23.18  23.620  23.1000  23.42   9178696.0        True\n",
            "2024-12-13  23.52  23.630  23.2800  23.40   4443501.0        True\n",
            "\n",
            "Social Media Sentiment:\n",
            "{'bullish': 0.6, 'bearish': 0.3, 'neutral': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alpha_vantage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYjws_ZGAVhd",
        "outputId": "475a0c9f-2899-4afd-e7bd-59bd9c1aa6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting alpha_vantage\n",
            "  Downloading alpha_vantage-3.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from alpha_vantage) (3.11.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from alpha_vantage) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->alpha_vantage) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->alpha_vantage) (2025.1.31)\n",
            "Downloading alpha_vantage-3.0.0-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: alpha_vantage\n",
            "Successfully installed alpha_vantage-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main code"
      ],
      "metadata": {
        "id": "sU4tfEFVzcG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "from textblob import TextBlob\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "\n",
        "\n",
        "#stock data collection\n",
        "ALPHA_VANTAGE_API_KEY = \"ED3T9IQN5OD495QC\"\n",
        "STOCK_SYMBOL = \"AAPL\"\n",
        "\n",
        "ts = TimeSeries(key=ALPHA_VANTAGE_API_KEY, output_format='pandas')\n",
        "data, meta_data = ts.get_daily(symbol=STOCK_SYMBOL, outputsize='compact')\n",
        "\n",
        "\n",
        "data.to_csv(\"stock_data.csv\") # storing stock data as a CSV file\n",
        "print(\"Stock data saved successfully.\")\n",
        "\n",
        "# StockTwits Data\n",
        "def fetch_stocktwits_data(symbol):\n",
        "    url = f\"https://api.stocktwits.com/api/2/streams/symbol/{symbol}.json\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def analyze_sentiment(messages):\n",
        "    sentiments = []\n",
        "    for msg in messages:\n",
        "        text = msg['body']\n",
        "        sentiment = TextBlob(text).sentiment.polarity\n",
        "        sentiments.append({'timestamp': msg['created_at'], 'text': text, 'sentiment_score': sentiment})\n",
        "    return sentiments\n",
        "\n",
        "stocktwits_data = fetch_stocktwits_data(\"TCS\")\n",
        "if stocktwits_data:\n",
        "    messages = stocktwits_data['messages']\n",
        "    sentiment_analysis = analyze_sentiment(messages)\n",
        "    df_sentiment = pd.DataFrame(sentiment_analysis)\n",
        "    df_sentiment.to_csv(\"sentiment_data.csv\", index=False)\n",
        "    print(\"Sentiment data saved successfully.\")\n",
        "else:\n",
        "    print(\"Failed to fetch StockTwits data.\")\n",
        "\n",
        "\n",
        "def analyze_news_sentiment(news_text):\n",
        "    return TextBlob(news_text).sentiment.polarity\n",
        "\n",
        "news_text_sample = \"Stock markets rally as tech stocks soar.\"\n",
        "print(\"Sample News Sentiment Score:\", analyze_news_sentiment(news_text_sample))\n"
      ],
      "metadata": {
        "id": "O2hbSfuShXEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36301a50-595c-4d54-9a35-06d167283768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock data saved successfully.\n",
            "Failed to fetch StockTwits data.\n",
            "Sample News Sentiment Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElDqIeTsr4b1",
        "outputId": "74a8e880-864f-41ee-cc1c-009b990ca01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=208730de486f11e99ae97bbc567df4a5a2853405ec2ad5fb060084893a8550ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection\n",
        "1. Fetch daily stock data using Alpha Vantage API\n",
        "2. Analyze sentiment of messages\n",
        "3. Analyze sentiment of news text\n",
        "\n"
      ],
      "metadata": {
        "id": "JUjm350A2Gko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "from textblob import TextBlob\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "import feedparser\n",
        "\n",
        "ALPHA_VANTAGE_API_KEY = \"ED3T9IQN5OD495QC\"\n",
        "STOCK_SYMBOL = \"AAPL\"\n",
        "STOCKTWITS_API_URL = \"https://api.stocktwits.com/api/2/streams/symbol/{symbol}.json\"\n",
        "\n",
        "def fetch_stock_data(symbol, api_key):\n",
        "    try:\n",
        "        ts = TimeSeries(key=api_key, output_format='pandas')\n",
        "        data, meta_data = ts.get_daily(symbol=symbol, outputsize='compact')\n",
        "        data.to_csv(\"stock_data.csv\")\n",
        "        print(f\"\\nStock data for {symbol} saved successfully.\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching stock data: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_stocktwits_data(symbol):\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        url = STOCKTWITS_API_URL.format(symbol=symbol)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            print(f\"Successfully fetched StockTwits data for {symbol}\")\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"Failed to fetch StockTwits data. Status code: {response.status_code}\")\n",
        "            print(f\"Response content: {response.text}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error in fetching StockTwits data: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_sentiment(messages):\n",
        "    sentiments = []\n",
        "    for msg in messages:\n",
        "        text = msg.get('body', '')\n",
        "        sentiment = TextBlob(text).sentiment.polarity\n",
        "        sentiments.append({\n",
        "            'timestamp': msg.get('created_at', 'N/A'),\n",
        "            'text': text,\n",
        "            'sentiment_score': sentiment\n",
        "        })\n",
        "    return sentiments\n",
        "\n",
        "def get_google_news_rss(stock_name):\n",
        "    url = f\"https://news.google.com/rss/search?q={stock_name}+stock\"\n",
        "    feed = feedparser.parse(url)\n",
        "\n",
        "    news_list = []\n",
        "    for entry in feed.entries[:5]:  # Fetch top 5 news articles\n",
        "        news_list.append({\"title\": entry.title, \"link\": entry.link})\n",
        "\n",
        "    return news_list\n",
        "\n",
        "news_data = get_google_news_rss(\"TCS\")\n",
        "for news in news_data:\n",
        "    print(\"\\n\",news[\"title\"], \"-\", news[\"link\"])\n",
        "\n",
        "def analyze_news_sentiment(news_data):\n",
        "    return TextBlob(news_data).sentiment.polarity\n",
        "\n",
        "def main():\n",
        "    stock_data = fetch_stock_data(STOCK_SYMBOL, ALPHA_VANTAGE_API_KEY)\n",
        "    stocktwits_data = fetch_stocktwits_data(STOCK_SYMBOL)\n",
        "\n",
        "    if stocktwits_data and 'messages' in stocktwits_data:\n",
        "        sentiment_analysis = analyze_sentiment(stocktwits_data['messages'])\n",
        "        df_sentiment = pd.DataFrame(sentiment_analysis)\n",
        "        df_sentiment.to_csv(\"sentiment_data.csv\", index=False)\n",
        "        print(\"Sentiment data saved successfully.\")\n",
        "    else:\n",
        "        print(\"No messages found in StockTwits data.\")\n",
        "    news_text_sample = \"Stock markets rally as tech stocks soar.\"\n",
        "    print(\"Sample News Sentiment Score:\", analyze_news_sentiment(news_text_sample))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9meFyDis_mnP",
        "outputId": "f7d6ea49-327e-404a-8431-eb620acd0bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " TCS, Infosys, HDFC Bank, HCL Tech among 5 key stocks to declare interim dividend in April 2025 - Mint - https://news.google.com/rss/articles/CBMi8wFBVV95cUxQOXJ0TEd1UWVmTXRSS0UxYThjNnJjaS1SWWJReGdLS1RYMVAwR2pwaE00OExTQzhKeFdUckVYTlh3dFNZQ19vQkV2TG5zdVRqd19ZVXVaRlF4TFk4MGZBR2EtV3YzZXI5RkNzU01icHZCbVo0YTdhemstVU1EN2wwaUdUYnFINk5DRTlsbk95UTJfeFQ0WUYyVkxFRFZKaG5Cc212QVFycjJxVFZVRkZuN09oQjBPRF9fTmc5RzlrZG1tMW1HMjJuUHpHWWJSU3RtT2NsOWpPbkJtQ3hxMUJWRTA5c0lnLTE3WXhGS21tXzQ3OUXSAfgBQVVfeXFMTXd6aFRBUEROLW95SDdmeVU0dldqREJxdGpIOUJYV2x4XzFYS21zNlNZa3JmR1o5ZXpYVEQxSVB3NVVBblRWb0haenFVSGJqdHFONzllUlFXcVhINlplN2Vicm5kQS1oZlZIbFo5ZHltNVVscDBIdXl0Nm5EY1Zud3UtRmZMQmF4SHZlaWo4NGdCVEFDU1RyNlUtcDE2cGxTdGdfR01TdGFrTG8wVU93TlhLTktFNXVHZE50ejN0VUNTNnNxRzh4RmlwejJQNkdoTmNFUWtYZTYtcmR3cGdhazdibnJ6a0pmS3pyR0s1TGU3WHhiZTE3Tjg?oc=5\n",
            "\n",
            " 32% target price slash! Goldman flags big risks for TCS, Infosys & other IT stocks amid US worries - The Economic Times - https://news.google.com/rss/articles/CBMi3AFBVV95cUxPdVdENUlnZEhJb213YkxKX2R0RE51Sm9FRGlRbGhwQlNXR3dfNVRpeGZNVHZ3b09CWGhEak9hdmlyOTFzekZ6dGdORVhhNWZIR3FjOG9FTkNoR0ZfNWxVS2RQQnhRYjgzSUZPZGdRTjc2bUdhUG1SWWc3OVdOVF9VRjNZeURCWjNSQlo4LVRKcVVYb0lGVmpRR1NkUFJEMThQR1dvUzVIZDNFczQ4WmFiNzhLbXdUT0FrUWVkNmpDbl9CRWlzaC1vTmk1M2huQnRvWDVIREsxMU8tOFU50gHiAUFVX3lxTFBteEZETU1iQkNMUUZoSzZlUURqaTdVNEVCVlEyZmdLdUZwVmpDS2FrSjEyZ1ZJSzhxVldETVpzYjVGbFhaZGVXVTdXazZzZHY5c3VYSUM4U2hGejZHQ0pKNFRkTjB4R1hvdkRnbFlpZjhkLVl0U3NBZVpxeS1fMU1hTUpTU190Ym5YaTZ6UzBCQXUwNE0zbWV0VkJ3cXlNRTZuVzA1R0VueUJqTWo5SHJVZFJkcXhUWWpQOXRKWXpud29JU1BEQkx4VFRjd1YzbWUzNEMwWk9KbU5LcjgxYTdkcHc?oc=5\n",
            "\n",
            " LTIMindtree shares downgraded by Goldman Sachs, price targets cut for TCS, Infosys - CNBCTV18 - https://news.google.com/rss/articles/CBMi0AFBVV95cUxPaFFTTmNJN0VsQVJBZENhTFRuMjcwZWtsUGswc0dMRTQtbmVCTDVWMFBzRlNXSHN6Q0t6SzR4N1pCeUFkRmZERjFsOWJBckN4UUhsaXU2bWd4R0dzSFN2LUt2UjB3amhTR25uOVA4UndRbmhhNndtMDRfblZaTEdVQUZMaUdqQ2pkTk5VcDEzOGtmdmRYZ2xSTUFkakZBbWx0aUpsbGo0blBEYXdEU3hrR3FfSlBDZHNNZ2RCSC14RWFvSlNWM0REcjA5QnEzWEVs0gHWAUFVX3lxTFBQdHFrZ3M5ZC14ajZkSkJnOTZGNnprNWhvQ09zcDFqUmpHS1pNLWk0TVYwOGFLM0hvenBUSnpLSFRoWHdMcHZhc3ZnVEhGXzJENkJfMXVQdDhZcnhUb1k5UzNqTk1YMTREOEprTDdfTkxxa2xBZE04S2FkRjZIdktlQUNCdngyQlljOWZJNzRQR3FxWXdYU1lhQ1pOazRJSnhBRjllMWN0T0tUbmlLTENTWDlkNkc2RWtOem14Ull3bVhKb0FtOGFPR2tsalZBcFp0NkxBNFE?oc=5\n",
            "\n",
            " IT Stocks, Wipro, Infosys, TCS Share Price Highlights: Wipro ends nearly 2% higher, Infosys, TCS, HCL Tech shares end flat - BusinessLine - https://news.google.com/rss/articles/CBMi2gFBVV95cUxOcldqU0kzVHhmVkRQRVhXVzhiMHA2WDRZM3htb2hxQVpqWmxFVGp1MTV6OWlIZFg3YnFwUmRlY2RnM1BnLVJvaVdUS2FCXzlHOHk3QzdubW1sS0JxeVhtcFVmN3Rqb3RNalVHRHVzaFZ2eGp0RGdKTUlJTzUzTFBOaHFkaWwzN1J5LVdoMXgyWmZOS09zMVd0MmxzZExUQUJPOHdiZ2tSMnF3R2tXWURzaEFKNVpjd21jaEI3R2daU0gzZmI1S0tva3RjV2xYUjZyd2xNZTZhS0o3dw?oc=5\n",
            "\n",
            " TCS, Infosys, HDFC Bank, HCL Tech among 5 key stocks to declare interim dividend in April 2025 - MSN - https://news.google.com/rss/articles/CBMi2wFBVV95cUxQUzN6YlQtdEJ6WDJUTFN5aFdCaFlxdGxSeXRoZzZvOFlfU3cyZjJITGUzOG5aRV9WXzBWd2tDNmZrbi1tWDg2YjNzZGRGNy1wczl2eDRnNG5qZV9Tb3V1b0dtWEZGdzUtUVhQTERGTC1KU2tUbGhpNGF0cEd2UURCcFZkdWhyNVhRN1lPYTFNVHpZSUJIbWJBQTJycWhNV0lOQTVfLWI4bVB1MzJjeDdDMFozTkd2NmFybEd6ZmpEc3lSUUN5M0Z2XzN0X0VFekt2dHY3dElHbnFEdzQ?oc=5\n",
            "\n",
            "Stock data for AAPL saved successfully.\n",
            "Successfully fetched StockTwits data for AAPL\n",
            "Sentiment data saved successfully.\n",
            "Sample News Sentiment Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# April Monthly Progress"
      ],
      "metadata": {
        "id": "2WKoLnHCfdAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy requests matplotlib seaborn scikit-learn xgboost nltk tweepy newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSBOEEPBfi75",
        "outputId": "ea8682dc-726f-431e-86ec-11a60b2bb067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.4.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=25aedf59c847e31eb8bf0b728b48d24e5bca4d684de321743d852573de4bf0c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=0178cea5ec0528754f14ea38265a29547dbb9d4881c743e2604434869f7083b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=04bcb843f24e0647abd067a130af9e4a2ce2ce4c4f60338c8f70ba063a96926c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=f7f572c638458351f03819e24aa55262a4700b6b29cd53ef7be7163e2bf9246a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emruFvD6gcQX",
        "outputId": "1fdadfef-ab8c-4c7f-c889-b21e1ca12380"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.2.1)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.4.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.4.26)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=b549c4b90c8aa9e9c18cf97147ab5e5991971044dbed1834b20d005f7f5c3d29\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=7d1fbd6241d25a601210055dd028857298e8d0a74a05082403781b8eadff4355\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=59cc6eb90ed935a4e63c9925a0542b49d74e94b038a0521d579a2f4dd00385f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=86b0fceed8d1a277dfc7246546a7d03a086858447aaeb1ed0333b7d99d3d0d67\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlZt9Cx4hmbU",
        "outputId": "c5fce2f4-8e4f-4660-834a-70cbea4b6849"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.4.0)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LONkpv4CmPjo",
        "outputId": "12016397-af4c-4c55-d26c-8a8cd9ef3771"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')  # Download the VADER lexicon for sentiment analysis"
      ],
      "metadata": {
        "id": "2Fvlr0jon6uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import tweepy\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# installing sentiment analyzer model\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "# initializing this instance\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Historical Stock Data Setup\n",
        "ALPHA_VANTAGE_API_KEY = \"PIG3WPABVKTBMH6Y\"\n",
        "\n",
        "# Twitter API setup\n",
        "TWITTER_API_KEY = \"ZkBtakhypMnFkI4dUzVo0QJTw\"\n",
        "TWITTER_API_SECRET = \"P1gqWUJsOFkjmkOsPuyB458xi8bwo4KZ1Cy0LbGXPQLxcR3v79\"\n",
        "TWITTER_ACCESS_TOKEN = \"1916142259252432899-t6yhBBktXrrexsMqb0DOR9ZgVDJJTB\"\n",
        "TWITTER_ACCESS_SECRET = \"L29j1XSBBYhJ614Ev2cE7Ukl3a0Vs5iwMxC9iyOEOJ3oR\"\n",
        "\n",
        "# Authenticate with Twitter API\n",
        "auth = tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_API_SECRET)\n",
        "auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_SECRET)\n",
        "twitter_api = tweepy.API(auth)\n",
        "\n",
        "# Class for stock manipulation detection\n",
        "class StockManipulationDetector:\n",
        "    def __init__(self, ticker_symbol, lookback_days=30):\n",
        "        self.ticker = ticker_symbol\n",
        "        self.lookback_days = lookback_days\n",
        "        self.stock_data = None\n",
        "        self.tweets = None\n",
        "        self.news = None\n",
        "        self.anomaly_model = None\n",
        "        self.manipulation_model = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def fetch_stock_data(self):\n",
        "        \"\"\"Fetch historical stock data from Alpha Vantage\"\"\"\n",
        "        print(f\"Fetching stock data for {self.ticker}...\")\n",
        "\n",
        "        # Daily data\n",
        "        url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={self.ticker}&outputsize=full&apikey={ALPHA_VANTAGE_API_KEY}\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        if \"Time Series (Daily)\" not in data:\n",
        "            print(\"Error fetching stock data. API response:\", data)\n",
        "            return False\n",
        "\n",
        "        df = pd.DataFrame(data[\"Time Series (Daily)\"]).T\n",
        "        df.columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "        df = df.astype(float)\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        df = df.sort_index()\n",
        "\n",
        "        # Calculate additional features\n",
        "        df['price_change'] = df['close'].pct_change()\n",
        "        df['volume_change'] = df['volume'].pct_change()\n",
        "        df['high_low_diff'] = df['high'] - df['low']\n",
        "        df['volatility'] = df['price_change'].rolling(window=5).std()\n",
        "\n",
        "        # Rolling statistics\n",
        "        df['price_mean_5d'] = df['close'].rolling(window=5).mean()\n",
        "        df['volume_mean_5d'] = df['volume'].rolling(window=5).mean()\n",
        "        df['price_std_5d'] = df['close'].rolling(window=5).std()\n",
        "        df['volume_std_5d'] = df['volume'].rolling(window=5).std()\n",
        "\n",
        "        # Z-scores for anomaly detection\n",
        "        df['price_z_score'] = (df['close'] - df['price_mean_5d']) / df['price_std_5d']\n",
        "        df['volume_z_score'] = (df['volume'] - df['volume_mean_5d']) / df['volume_std_5d']\n",
        "\n",
        "        # Momentum indicators\n",
        "        df['price_momentum'] = df['close'] - df['close'].shift(5)\n",
        "        df['volume_momentum'] = df['volume'] - df['volume'].shift(5)\n",
        "\n",
        "        # Filter to relevant period and drop NAs\n",
        "        df = df.iloc[-self.lookback_days*2:]\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        self.stock_data = df\n",
        "        print(f\"Fetched {len(df)} days of stock data\")\n",
        "        return True\n",
        "\n",
        "    # Simplified Twitter sentiment collection using text files or CSV instead of API\n",
        "    def fetch_tweets_alternative(self, ticker):\n",
        "        \"\"\"Alternative method when Twitter API is unavailable\"\"\"\n",
        "        print(f\"Using alternative sentiment data for {ticker}...\")\n",
        "\n",
        "        # Create synthetic sentiment data based on stock price movements\n",
        "        # This is a fallback when real Twitter data isn't available\n",
        "        if self.stock_data is not None:\n",
        "            dates = self.stock_data.index[-30:]  # Last 30 days\n",
        "\n",
        "            # Create synthetic tweet sentiment that somewhat follows price changes\n",
        "            # but with some randomness and lag\n",
        "            price_changes = self.stock_data['price_change'].values[-32:-2]  # Lagged by 2 days\n",
        "\n",
        "            synthetic_tweet_data = []\n",
        "\n",
        "            for i, date in enumerate(dates):\n",
        "                # Base sentiment on lagged price changes with noise\n",
        "                base_sentiment = price_changes[i] * 5  # Scale up for sentiment range\n",
        "                sentiment = min(max(base_sentiment + np.random.normal(0, 0.3), -1), 1)  # Bound between -1 and 1\n",
        "\n",
        "                # Create more tweets on volatile days\n",
        "                tweet_count = int(50 + abs(sentiment) * 200 + np.random.normal(0, 20))\n",
        "                tweet_count = max(10, tweet_count)  # At least 10 tweets\n",
        "\n",
        "                synthetic_tweet_data.append({\n",
        "                    'date': date,\n",
        "                    'tweet_sentiment_mean': sentiment,\n",
        "                    'tweet_sentiment_std': 0.3 + abs(sentiment) * 0.2,\n",
        "                    'tweet_count': tweet_count,\n",
        "                    'retweet_count': tweet_count * 3,\n",
        "                    'favorite_count': tweet_count * 5\n",
        "                })\n",
        "\n",
        "            self.tweets = pd.DataFrame(synthetic_tweet_data)\n",
        "            self.tweets.set_index('date', inplace=True)\n",
        "            print(f\"Created synthetic sentiment data for {ticker}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    # Simplified news collection using text files or CSV instead of web scraping\n",
        "    def fetch_news_alternative(self, ticker):\n",
        "        \"\"\"Alternative method when news scraping is blocked\"\"\"\n",
        "        print(f\"Using alternative news data for {ticker}...\")\n",
        "\n",
        "        # Create synthetic news data based on stock movements\n",
        "        if self.stock_data is not None:\n",
        "            dates = self.stock_data.index[-30:]  # Last 30 days\n",
        "\n",
        "            # Create news sentiment that somewhat follows price trends\n",
        "            # but with occasional contrarian articles\n",
        "            synthetic_news_data = []\n",
        "\n",
        "            for date in dates:\n",
        "                # Get price data for this date if available\n",
        "                if date in self.stock_data.index:\n",
        "                    price_change = self.stock_data.loc[date, 'price_change']\n",
        "\n",
        "                    # Occasionally have contrarian news\n",
        "                    contrarian = np.random.random() > 0.7\n",
        "\n",
        "                    if contrarian:\n",
        "                        # News sentiment opposite to price movement\n",
        "                        sentiment = -price_change * 3\n",
        "                    else:\n",
        "                        # News sentiment aligned with price movement\n",
        "                        sentiment = price_change * 3\n",
        "\n",
        "                    sentiment = min(max(sentiment + np.random.normal(0, 0.2), -1), 1)\n",
        "\n",
        "                    # More news on days with bigger price moves\n",
        "                    news_count = int(2 + abs(price_change) * 20 + np.random.normal(0, 1))\n",
        "                    news_count = max(1, news_count)  # At least 1 news item\n",
        "\n",
        "                    synthetic_news_data.append({\n",
        "                        'date': date,\n",
        "                        'news_sentiment_mean': sentiment,\n",
        "                        'news_sentiment_std': 0.2,\n",
        "                        'news_count': news_count\n",
        "                    })\n",
        "\n",
        "            self.news = pd.DataFrame(synthetic_news_data)\n",
        "            self.news.set_index('date', inplace=True)\n",
        "            print(f\"Created synthetic news data for {ticker}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def analyze_sentiment(self):\n",
        "        \"\"\"Analyze sentiment from tweets and news, aggregate by day\"\"\"\n",
        "        print(\"Analyzing sentiment data...\")\n",
        "\n",
        "        # Create date ranges for the period we're analyzing\n",
        "        end_date = datetime.now().date()\n",
        "        start_date = end_date - timedelta(days=self.lookback_days)\n",
        "        date_range = pd.date_range(start=start_date, end=end_date)\n",
        "\n",
        "        # Initialize sentiment DataFrames\n",
        "        sentiment_daily = pd.DataFrame(index=date_range)\n",
        "\n",
        "        # Process tweets sentiment\n",
        "        if self.tweets is not None and not self.tweets.empty:\n",
        "            # Convert to datetime and extract date\n",
        "            self.tweets['date'] = self.tweets['created_at'].dt.date\n",
        "\n",
        "            # Group by date and calculate metrics\n",
        "            tweet_sentiment = self.tweets.groupby('date').agg({\n",
        "                'sentiment': ['mean', 'std', 'count'],\n",
        "                'retweet_count': 'sum',\n",
        "                'favorite_count': 'sum'\n",
        "            })\n",
        "\n",
        "            tweet_sentiment.columns = ['tweet_sentiment_mean', 'tweet_sentiment_std',\n",
        "                                     'tweet_count', 'retweet_count', 'favorite_count']\n",
        "\n",
        "            # Convert index to datetime for proper joining\n",
        "            tweet_sentiment.index = pd.to_datetime(tweet_sentiment.index)\n",
        "\n",
        "            # Join with main sentiment DataFrame\n",
        "            sentiment_daily = sentiment_daily.join(tweet_sentiment)\n",
        "\n",
        "        # Process news sentiment\n",
        "        if self.news is not None and not self.news.empty:\n",
        "            # Convert to datetime and extract date\n",
        "            self.news['date'] = self.news['published_date'].dt.date\n",
        "\n",
        "            # Group by date and calculate metrics\n",
        "            news_sentiment = self.news.groupby('date').agg({\n",
        "                'sentiment': ['mean', 'std', 'count']\n",
        "            })\n",
        "\n",
        "            news_sentiment.columns = ['news_sentiment_mean', 'news_sentiment_std', 'news_count']\n",
        "\n",
        "            # Convert index to datetime for proper joining\n",
        "            news_sentiment.index = pd.to_datetime(news_sentiment.index)\n",
        "\n",
        "            # Join with main sentiment DataFrame\n",
        "            sentiment_daily = sentiment_daily.join(news_sentiment)\n",
        "\n",
        "        # Fill NaN values with 0 for calculation purposes\n",
        "        sentiment_daily = sentiment_daily.fillna(0)\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        if 'tweet_sentiment_mean' in sentiment_daily.columns:\n",
        "            sentiment_daily['tweet_sentiment_zscore'] = (\n",
        "                sentiment_daily['tweet_sentiment_mean'] -\n",
        "                sentiment_daily['tweet_sentiment_mean'].rolling(window=5).mean()\n",
        "            ) / sentiment_daily['tweet_sentiment_mean'].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        if 'news_sentiment_mean' in sentiment_daily.columns:\n",
        "            sentiment_daily['news_sentiment_zscore'] = (\n",
        "                sentiment_daily['news_sentiment_mean'] -\n",
        "                sentiment_daily['news_sentiment_mean'].rolling(window=5).mean()\n",
        "            ) / sentiment_daily['news_sentiment_mean'].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        # Add sentiment momentum (change from previous day)\n",
        "        for col in ['tweet_sentiment_mean', 'news_sentiment_mean']:\n",
        "            if col in sentiment_daily.columns:\n",
        "                sentiment_daily[f'{col}_change'] = sentiment_daily[col].diff()\n",
        "\n",
        "        # Add volume change metrics\n",
        "        for col in ['tweet_count', 'news_count']:\n",
        "            if col in sentiment_daily.columns:\n",
        "                sentiment_daily[f'{col}_change'] = sentiment_daily[col].pct_change()\n",
        "                sentiment_daily[f'{col}_zscore'] = (\n",
        "                    sentiment_daily[col] -\n",
        "                    sentiment_daily[col].rolling(window=5).mean()\n",
        "                ) / sentiment_daily[col].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        return sentiment_daily\n",
        "\n",
        "    def integrate_data(self):\n",
        "        \"\"\"Integrate stock data with sentiment analysis\"\"\"\n",
        "        print(\"Integrating market and sentiment data...\")\n",
        "\n",
        "        # Get sentiment data\n",
        "        sentiment_daily = self.analyze_sentiment()\n",
        "\n",
        "        # Make sure stock_data index is datetime\n",
        "        self.stock_data.index = pd.to_datetime(self.stock_data.index)\n",
        "\n",
        "        # Merge sentiment with stock data\n",
        "        merged_data = self.stock_data.join(sentiment_daily, how='left')\n",
        "\n",
        "        # Fill missing values\n",
        "        merged_data = merged_data.fillna(0)\n",
        "\n",
        "        # Calculate correlations between sentiment and price/volume changes\n",
        "        # These correlations can help identify manipulation\n",
        "        if 'tweet_sentiment_mean' in merged_data.columns:\n",
        "            merged_data['tweet_price_corr'] = merged_data['tweet_sentiment_mean'].rolling(window=5).corr(merged_data['price_change'])\n",
        "            merged_data['tweet_volume_corr'] = merged_data['tweet_sentiment_mean'].rolling(window=5).corr(merged_data['volume_change'])\n",
        "\n",
        "        if 'news_sentiment_mean' in merged_data.columns:\n",
        "            merged_data['news_price_corr'] = merged_data['news_sentiment_mean'].rolling(window=5).corr(merged_data['price_change'])\n",
        "            merged_data['news_volume_corr'] = merged_data['news_sentiment_mean'].rolling(window=5).corr(merged_data['volume_change'])\n",
        "\n",
        "        # Add features that might indicate manipulation\n",
        "        # 1. Abnormal price changes with high sentiment but low news (pump)\n",
        "        if 'tweet_count' in merged_data.columns and 'news_count' in merged_data.columns:\n",
        "            merged_data['pump_indicator'] = (\n",
        "                (merged_data['price_z_score'] > 1.5) &\n",
        "                (merged_data['tweet_sentiment_zscore'] > 1.5) &\n",
        "                (merged_data['news_count'] < merged_data['news_count'].mean())\n",
        "            ).astype(int)\n",
        "\n",
        "        # 2. High volume with negative sentiment divergence (dump)\n",
        "        if 'tweet_sentiment_zscore' in merged_data.columns:\n",
        "            merged_data['dump_indicator'] = (\n",
        "                (merged_data['volume_z_score'] > 1.5) &\n",
        "                (merged_data['price_change'] < 0) &\n",
        "                (merged_data['tweet_sentiment_zscore'] < -1.5)\n",
        "            ).astype(int)\n",
        "\n",
        "        # Keep only the most recent lookback days\n",
        "        merged_data = merged_data.iloc[-self.lookback_days:]\n",
        "\n",
        "        return merged_data\n",
        "\n",
        "    def train_anomaly_model(self, data):\n",
        "        \"\"\"Train isolation forest model for anomaly detection\"\"\"\n",
        "        print(\"Training anomaly detection model...\")\n",
        "\n",
        "        # Select features for anomaly detection\n",
        "        feature_cols = [\n",
        "            'price_z_score', 'volume_z_score', 'volatility',\n",
        "            'high_low_diff', 'price_momentum', 'volume_momentum'\n",
        "        ]\n",
        "\n",
        "        # Add sentiment features if available\n",
        "        sentiment_features = [\n",
        "            'tweet_sentiment_zscore', 'news_sentiment_zscore',\n",
        "            'tweet_count_zscore', 'news_count_zscore',\n",
        "            'tweet_price_corr', 'news_price_corr'\n",
        "        ]\n",
        "\n",
        "        for feature in sentiment_features:\n",
        "            if feature in data.columns:\n",
        "                feature_cols.append(feature)\n",
        "\n",
        "        # Get feature subset that exists in the data\n",
        "        valid_features = [col for col in feature_cols if col in data.columns]\n",
        "\n",
        "        if not valid_features:\n",
        "            print(\"No valid features found for anomaly detection\")\n",
        "            return False\n",
        "\n",
        "        # Extract features\n",
        "        X = data[valid_features].fillna(0)\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Train isolation forest\n",
        "        self.anomaly_model = IsolationForest(\n",
        "            n_estimators=100,\n",
        "            contamination=0.05,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        self.anomaly_model.fit(X_scaled)\n",
        "\n",
        "        # Add anomaly scores to the data\n",
        "        data['anomaly_score'] = self.anomaly_model.decision_function(X_scaled)\n",
        "        data['is_anomaly'] = self.anomaly_model.predict(X_scaled)\n",
        "\n",
        "        # Convert prediction to binary (1 for normal, -1 for anomaly)\n",
        "        data['is_anomaly'] = (data['is_anomaly'] == -1).astype(int)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def train_manipulation_model(self, data):\n",
        "        \"\"\"Train XGBoost model to classify potential manipulation\"\"\"\n",
        "        print(\"Training manipulation detection model...\")\n",
        "\n",
        "        # For a real system, you would have labeled data of known manipulation cases\n",
        "        # Since we don't have labels, we'll create synthetic ones based on our indicators\n",
        "\n",
        "        # Define manipulation as days with anomalies and either pump or dump indicators\n",
        "        if 'pump_indicator' in data.columns and 'dump_indicator' in data.columns:\n",
        "            data['potential_manipulation'] = (\n",
        "                (data['is_anomaly'] == 1) &\n",
        "                ((data['pump_indicator'] == 1) | (data['dump_indicator'] == 1))\n",
        "            ).astype(int)\n",
        "        else:\n",
        "            # Fallback to just anomalies if we don't have the indicators\n",
        "            data['potential_manipulation'] = data['is_anomaly']\n",
        "\n",
        "        # Select features for the classifier\n",
        "        feature_cols = [\n",
        "            'price_z_score', 'volume_z_score', 'volatility',\n",
        "            'high_low_diff', 'price_momentum', 'volume_momentum'\n",
        "        ]\n",
        "\n",
        "        # Add sentiment features if available\n",
        "        sentiment_features = [\n",
        "            'tweet_sentiment_mean', 'news_sentiment_mean',\n",
        "            'tweet_count', 'news_count',\n",
        "            'tweet_sentiment_zscore', 'news_sentiment_zscore',\n",
        "            'tweet_price_corr', 'news_price_corr'\n",
        "        ]\n",
        "\n",
        "        for feature in sentiment_features:\n",
        "            if feature in data.columns:\n",
        "                feature_cols.append(feature)\n",
        "\n",
        "        # Get feature subset that exists in the data\n",
        "        valid_features = [col for col in feature_cols if col in data.columns]\n",
        "\n",
        "        if not valid_features:\n",
        "            print(\"No valid features found for manipulation model\")\n",
        "            return data\n",
        "\n",
        "        # Extract features and target\n",
        "        X = data[valid_features].fillna(0)\n",
        "        y = data['potential_manipulation']\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Train XGBoost model\n",
        "        self.manipulation_model = xgb.XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=3,\n",
        "            random_state=42,\n",
        "            use_label_encoder=False,\n",
        "            eval_metric='logloss'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Only train if we have both classes represented\n",
        "            if len(y.unique()) > 1:\n",
        "                self.manipulation_model.fit(X_scaled, y)\n",
        "\n",
        "                # Add predictions to the data\n",
        "                data['manipulation_probability'] = self.manipulation_model.predict_proba(X_scaled)[:, 1]\n",
        "                data['predicted_manipulation'] = self.manipulation_model.predict(X_scaled)\n",
        "\n",
        "                # Feature importance\n",
        "                feature_importance = pd.DataFrame({\n",
        "                    'feature': valid_features,\n",
        "                    'importance': self.manipulation_model.feature_importances_\n",
        "                }).sort_values('importance', ascending=False)\n",
        "\n",
        "                print(\"\\nTop manipulation indicators:\")\n",
        "                print(feature_importance.head(5))\n",
        "            else:\n",
        "                print(\"Not enough variation in the target variable to train classifier\")\n",
        "                data['manipulation_probability'] = 0\n",
        "                data['predicted_manipulation'] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training manipulation model: {e}\")\n",
        "            data['manipulation_probability'] = 0\n",
        "            data['predicted_manipulation'] = 0\n",
        "\n",
        "        return data\n",
        "\n",
        "    def detect_manipulation(self, demo_mode=True):\n",
        "        \"\"\"Main method to run the entire detection pipeline\"\"\"\n",
        "        # Fetch data\n",
        "        if not self.fetch_stock_data():\n",
        "            print(\"Failed to fetch stock data. Aborting.\")\n",
        "            return None\n",
        "\n",
        "        if demo_mode:\n",
        "            # Use alternative data sources that don't require APIs\n",
        "            self.fetch_tweets_alternative(self.ticker)\n",
        "            self.fetch_news_alternative(self.ticker)\n",
        "        else:\n",
        "            # Try to use actual APIs (may fail with current limitations)\n",
        "            self.fetch_tweets()\n",
        "            self.fetch_news()\n",
        "\n",
        "    # Rest of the method remains the same...\n",
        "\n",
        "    def display_results(self, data):\n",
        "        \"\"\"Display detection results and visualizations\"\"\"\n",
        "        if data is None or data.empty:\n",
        "            print(\"No data available to display results\")\n",
        "            return\n",
        "\n",
        "        # Print summary of detected manipulations\n",
        "        print(\"\\n----- MANIPULATION DETECTION SUMMARY -----\")\n",
        "\n",
        "        # Filter to just the most recent period\n",
        "        recent_data = data.iloc[-self.lookback_days:]\n",
        "\n",
        "        # Count days with potential manipulation\n",
        "        if 'predicted_manipulation' in recent_data.columns:\n",
        "            manipulation_days = recent_data[recent_data['predicted_manipulation'] == 1]\n",
        "            n_manipulation_days = len(manipulation_days)\n",
        "\n",
        "            print(f\"Detected potential manipulation on {n_manipulation_days} days out of {len(recent_data)} analyzed.\")\n",
        "\n",
        "            if n_manipulation_days > 0:\n",
        "                print(\"\\nDates with suspected manipulation:\")\n",
        "                for date, row in manipulation_days.iterrows():\n",
        "                    features = []\n",
        "\n",
        "                    # Add indicators that triggered the alert\n",
        "                    if row['price_z_score'] > 1.5:\n",
        "                        features.append(f\"Abnormal price (z={row['price_z_score']:.2f})\")\n",
        "                    if row['volume_z_score'] > 1.5:\n",
        "                        features.append(f\"Abnormal volume (z={row['volume_z_score']:.2f})\")\n",
        "                    if 'tweet_sentiment_zscore' in row and row['tweet_sentiment_zscore'] > 1.5:\n",
        "                        features.append(f\"Abnormal social sentiment (z={row['tweet_sentiment_zscore']:.2f})\")\n",
        "                    if 'pump_indicator' in row and row['pump_indicator'] == 1:\n",
        "                        features.append(\"Pump pattern\")\n",
        "                    if 'dump_indicator' in row and row['dump_indicator'] == 1:\n",
        "                        features.append(\"Dump pattern\")\n",
        "\n",
        "                    print(f\"  {date.date()}: {', '.join(features)}\")\n",
        "        else:\n",
        "            print(\"Manipulation classification not available.\")\n",
        "\n",
        "        # Plot results\n",
        "        try:\n",
        "            self.plot_results(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating plots: {e}\")\n",
        "\n",
        "    def plot_results(self, data):\n",
        "        \"\"\"Create visualizations of the detection results\"\"\"\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(14, 18), sharex=True)\n",
        "\n",
        "        # Stock price with anomaly highlighting\n",
        "        ax0 = axes[0]\n",
        "        ax0.set_title(f\"{self.ticker} Stock Price with Anomaly Detection\", fontsize=14)\n",
        "        ax0.plot(data.index, data['close'], label='Close Price', color='blue')\n",
        "\n",
        "        # Highlight anomalies if available\n",
        "        if 'is_anomaly' in data.columns:\n",
        "            anomaly_days = data[data['is_anomaly'] == 1]\n",
        "            ax0.scatter(anomaly_days.index, anomaly_days['close'],\n",
        "                      color='red', label='Anomalies', zorder=5)\n",
        "\n",
        "        # Highlight manipulation if available\n",
        "        if 'predicted_manipulation' in data.columns:\n",
        "            manip_days = data[data['predicted_manipulation'] == 1]\n",
        "            ax0.scatter(manip_days.index, manip_days['close'],\n",
        "                      color='darkred', marker='X', s=100,\n",
        "                      label='Potential Manipulation', zorder=10)\n",
        "\n",
        "        ax0.set_ylabel('Price ($)')\n",
        "        ax0.legend()\n",
        "        ax0.grid(True, alpha=0.3)\n",
        "\n",
        "        # Volume plot\n",
        "        ax1 = axes[1]\n",
        "        ax1.set_title(f\"{self.ticker} Trading Volume\", fontsize=14)\n",
        "        ax1.bar(data.index, data['volume'], color='green', alpha=0.7, label='Volume')\n",
        "\n",
        "        # Highlight volume anomalies\n",
        "        volume_anomalies = data[data['volume_z_score'] > 1.5]\n",
        "        ax1.bar(volume_anomalies.index, volume_anomalies['volume'], color='orange', label='Volume Anomalies')\n",
        "\n",
        "        ax1.set_ylabel('Volume')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Sentiment plot\n",
        "        ax2 = axes[2]\n",
        "        ax2.set_title(\"Sentiment Analysis\", fontsize=14)\n",
        "\n",
        "        if 'tweet_sentiment_mean' in data.columns:\n",
        "            ax2.plot(data.index, data['tweet_sentiment_mean'],\n",
        "                   label='Social Sentiment', color='purple')\n",
        "\n",
        "        if 'news_sentiment_mean' in data.columns:\n",
        "            ax2.plot(data.index, data['news_sentiment_mean'],\n",
        "                   label='News Sentiment', color='brown')\n",
        "\n",
        "        # Add zero line\n",
        "        ax2.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
        "\n",
        "        ax2.set_ylabel('Sentiment Score')\n",
        "        ax2.set_xlabel('Date')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.ticker}_manipulation_analysis.png\")\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"\\nSaved visualization to {self.ticker}_manipulation_analysis.png\")\n",
        "\n",
        "def test_detector(ticker_symbol, demo_mode=True):\n",
        "    print(f\"\\n===== ANALYZING {ticker_symbol} =====\")\n",
        "    detector = StockManipulationDetector(ticker_symbol, lookback_days=30)\n",
        "    results = detector.detect_manipulation(demo_mode=demo_mode)\n",
        "    return results\n",
        "def evaluate_models(self, data):\n",
        "    \"\"\"Evaluate the performance of the anomaly and manipulation detection models\"\"\"\n",
        "    print(\"\\n----- MODEL EVALUATION -----\")\n",
        "\n",
        "    # For evaluation, we'll use our synthetic labels as \"ground truth\"\n",
        "    # In a real-world scenario, you would need human-labeled examples\n",
        "\n",
        "    # Evaluate anomaly detection\n",
        "    if 'is_anomaly' in data.columns and 'potential_manipulation' in data.columns:\n",
        "        # Use potential_manipulation as a proxy for ground truth\n",
        "        y_true = data['potential_manipulation']\n",
        "        y_pred = data['is_anomaly']\n",
        "\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        print(\"\\nAnomaly Detection Performance:\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Evaluate manipulation detection\n",
        "    if 'predicted_manipulation' in data.columns and 'potential_manipulation' in data.columns:\n",
        "        # Compare predictions against our synthetic \"ground truth\"\n",
        "        y_true = data['potential_manipulation']\n",
        "        y_pred = data['predicted_manipulation']\n",
        "\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        print(\"\\nManipulation Detection Performance:\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = (y_true == y_pred).mean()\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Return the overall accuracy\n",
        "        return accuracy\n",
        "\n",
        "    return None\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Test with a few stocks known for volatility and social media attention\n",
        "    tickers = [\"GME\", \"AMC\", \"TSLA\", \"AAPL\"]\n",
        "\n",
        "    for ticker in tickers:\n",
        "        test_detector(ticker)\n",
        "        # Add delay between API calls to avoid rate limits\n",
        "        time.sleep(2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Kmh-ucOLzaZU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "95dc47d6-516d-4bd3-c036-f1fe5c688cba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-dab7c3f61656>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[0;31m# Aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2475\u001b[0;31m \u001b[0m_downloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2476\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# decide where we're going to save things to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# variety of system-wide locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_models(self, data):\n",
        "    \"\"\"Evaluate the performance of the anomaly and manipulation detection models\"\"\"\n",
        "    print(\"\\n----- MODEL EVALUATION -----\")\n",
        "\n",
        "    # For evaluation, we'll use our synthetic labels as \"ground truth\"\n",
        "    # In a real-world scenario, you would need human-labeled examples\n",
        "\n",
        "    # Evaluate anomaly detection\n",
        "    if 'is_anomaly' in data.columns and 'potential_manipulation' in data.columns:\n",
        "        # Use potential_manipulation as a proxy for ground truth\n",
        "        y_true = data['potential_manipulation']\n",
        "        y_pred = data['is_anomaly']\n",
        "\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        print(\"\\nAnomaly Detection Performance:\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Evaluate manipulation detection\n",
        "    if 'predicted_manipulation' in data.columns and 'potential_manipulation' in data.columns:\n",
        "        # Compare predictions against our synthetic \"ground truth\"\n",
        "        y_true = data['potential_manipulation']\n",
        "        y_pred = data['predicted_manipulation']\n",
        "\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        print(\"\\nManipulation Detection Performance:\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = (y_true == y_pred).mean()\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Return the overall accuracy\n",
        "        return accuracy\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "fgGP0NI84M4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import tweepy\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# installing sentiment analyzer model\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "# initializing this instance\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Historical Stock Data Setup\n",
        "ALPHA_VANTAGE_API_KEY = \"PIG3WPABVKTBMH6Y\"\n",
        "\n",
        "# Twitter API setup\n",
        "TWITTER_API_KEY = \"ZkBtakhypMnFkI4dUzVo0QJTw\"\n",
        "TWITTER_API_SECRET = \"P1gqWUJsOFkjmkOsPuyB458xi8bwo4KZ1Cy0LbGXPQLxcR3v79\"\n",
        "TWITTER_ACCESS_TOKEN = \"1916142259252432899-t6yhBBktXrrexsMqb0DOR9ZgVDJJTB\"\n",
        "TWITTER_ACCESS_SECRET = \"L29j1XSBBYhJ614Ev2cE7Ukl3a0Vs5iwMxC9iyOEOJ3oR\"\n",
        "\n",
        "# Authenticate with Twitter API\n",
        "auth = tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_API_SECRET)\n",
        "auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_SECRET)\n",
        "twitter_api = tweepy.API(auth)\n",
        "\n",
        "# Class for stock manipulation detection\n",
        "class StockManipulationDetector:\n",
        "    def __init__(self, ticker_symbol, lookback_days=30):\n",
        "        self.ticker = ticker_symbol\n",
        "        self.lookback_days = lookback_days\n",
        "        self.stock_data = None\n",
        "        self.tweets = None\n",
        "        self.news = None\n",
        "        self.anomaly_model = None\n",
        "        self.manipulation_model = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def fetch_stock_data(self):\n",
        "        \"\"\"Fetch historical stock data from Alpha Vantage\"\"\"\n",
        "        print(f\"Fetching stock data for {self.ticker}...\")\n",
        "\n",
        "        # Daily data\n",
        "        url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={self.ticker}&outputsize=full&apikey={ALPHA_VANTAGE_API_KEY}\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        if \"Time Series (Daily)\" not in data:\n",
        "            print(\"Error fetching stock data. API response:\", data)\n",
        "            return False\n",
        "\n",
        "        df = pd.DataFrame(data[\"Time Series (Daily)\"]).T\n",
        "        df.columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "        df = df.astype(float)\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        df = df.sort_index()\n",
        "\n",
        "        # Calculate additional features\n",
        "        df['price_change'] = df['close'].pct_change()\n",
        "        df['volume_change'] = df['volume'].pct_change()\n",
        "        df['high_low_diff'] = df['high'] - df['low']\n",
        "        df['volatility'] = df['price_change'].rolling(window=5).std()\n",
        "\n",
        "        # Rolling statistics\n",
        "        df['price_mean_5d'] = df['close'].rolling(window=5).mean()\n",
        "        df['volume_mean_5d'] = df['volume'].rolling(window=5).mean()\n",
        "        df['price_std_5d'] = df['close'].rolling(window=5).std()\n",
        "        df['volume_std_5d'] = df['volume'].rolling(window=5).std()\n",
        "\n",
        "        # Z-scores for anomaly detection\n",
        "        df['price_z_score'] = (df['close'] - df['price_mean_5d']) / df['price_std_5d']\n",
        "        df['volume_z_score'] = (df['volume'] - df['volume_mean_5d']) / df['volume_std_5d']\n",
        "\n",
        "        # Momentum indicators\n",
        "        df['price_momentum'] = df['close'] - df['close'].shift(5)\n",
        "        df['volume_momentum'] = df['volume'] - df['volume'].shift(5)\n",
        "\n",
        "        # Filter to relevant period and drop NAs\n",
        "        df = df.iloc[-self.lookback_days*2:]\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        self.stock_data = df\n",
        "        print(f\"Fetched {len(df)} days of stock data\")\n",
        "        return True\n",
        "\n",
        "    # Simplified Twitter sentiment collection using text files or CSV instead of API\n",
        "    def fetch_tweets_alternative(self, ticker):\n",
        "        \"\"\"Alternative method when Twitter API is unavailable\"\"\"\n",
        "        print(f\"Using alternative sentiment data for {ticker}...\")\n",
        "\n",
        "        # Create synthetic sentiment data based on stock price movements\n",
        "        # This is a fallback when real Twitter data isn't available\n",
        "        if self.stock_data is not None:\n",
        "            dates = self.stock_data.index[-30:]  # Last 30 days\n",
        "\n",
        "            # Create synthetic tweet sentiment that somewhat follows price changes\n",
        "            # but with some randomness and lag\n",
        "            price_changes = self.stock_data['price_change'].values[-32:-2]  # Lagged by 2 days\n",
        "\n",
        "            synthetic_tweet_data = []\n",
        "\n",
        "            for i, date in enumerate(dates):\n",
        "                # Base sentiment on lagged price changes with noise\n",
        "                base_sentiment = price_changes[i] * 5  # Scale up for sentiment range\n",
        "                sentiment = min(max(base_sentiment + np.random.normal(0, 0.3), -1), 1)  # Bound between -1 and 1\n",
        "\n",
        "                # Create more tweets on volatile days\n",
        "                tweet_count = int(50 + abs(sentiment) * 200 + np.random.normal(0, 20))\n",
        "                tweet_count = max(10, tweet_count)  # At least 10 tweets\n",
        "\n",
        "                synthetic_tweet_data.append({\n",
        "                    'date': date,\n",
        "                    'tweet_sentiment_mean': sentiment,\n",
        "                    'tweet_sentiment_std': 0.3 + abs(sentiment) * 0.2,\n",
        "                    'tweet_count': tweet_count,\n",
        "                    'retweet_count': tweet_count * 3,\n",
        "                    'favorite_count': tweet_count * 5\n",
        "                })\n",
        "\n",
        "            self.tweets = pd.DataFrame(synthetic_tweet_data)\n",
        "            self.tweets.set_index('date', inplace=True)\n",
        "            print(f\"Created synthetic sentiment data for {ticker}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    # Simplified news collection using text files or CSV instead of web scraping\n",
        "    def fetch_news_alternative(self, ticker):\n",
        "        \"\"\"Alternative method when news scraping is blocked\"\"\"\n",
        "        print(f\"Using alternative news data for {ticker}...\")\n",
        "\n",
        "        # Create synthetic news data based on stock movements\n",
        "        if self.stock_data is not None:\n",
        "            dates = self.stock_data.index[-30:]  # Last 30 days\n",
        "\n",
        "            # Create news sentiment that somewhat follows price trends\n",
        "            # but with occasional contrarian articles\n",
        "            synthetic_news_data = []\n",
        "\n",
        "            for date in dates:\n",
        "                # Get price data for this date if available\n",
        "                if date in self.stock_data.index:\n",
        "                    price_change = self.stock_data.loc[date, 'price_change']\n",
        "\n",
        "                    # Occasionally have contrarian news\n",
        "                    contrarian = np.random.random() > 0.7\n",
        "\n",
        "                    if contrarian:\n",
        "                        # News sentiment opposite to price movement\n",
        "                        sentiment = -price_change * 3\n",
        "                    else:\n",
        "                        # News sentiment aligned with price movement\n",
        "                        sentiment = price_change * 3\n",
        "\n",
        "                    sentiment = min(max(sentiment + np.random.normal(0, 0.2), -1), 1)\n",
        "\n",
        "                    # More news on days with bigger price moves\n",
        "                    news_count = int(2 + abs(price_change) * 20 + np.random.normal(0, 1))\n",
        "                    news_count = max(1, news_count)  # At least 1 news item\n",
        "\n",
        "                    synthetic_news_data.append({\n",
        "                        'date': date,\n",
        "                        'news_sentiment_mean': sentiment,\n",
        "                        'news_sentiment_std': 0.2,\n",
        "                        'news_count': news_count\n",
        "                    })\n",
        "\n",
        "            self.news = pd.DataFrame(synthetic_news_data)\n",
        "            self.news.set_index('date', inplace=True)\n",
        "            print(f\"Created synthetic news data for {ticker}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def analyze_sentiment(self):\n",
        "        \"\"\"Analyze sentiment from tweets and news, aggregate by day\"\"\"\n",
        "        print(\"Analyzing sentiment data...\")\n",
        "\n",
        "        # Create date ranges for the period we're analyzing\n",
        "        end_date = datetime.now().date()\n",
        "        start_date = end_date - timedelta(days=self.lookback_days)\n",
        "        date_range = pd.date_range(start=start_date, end=end_date)\n",
        "\n",
        "        # Initialize sentiment DataFrames\n",
        "        sentiment_daily = pd.DataFrame(index=date_range)\n",
        "\n",
        "        # Process tweets sentiment\n",
        "        if self.tweets is not None and not self.tweets.empty:\n",
        "            # Convert to datetime and extract date\n",
        "            self.tweets['date'] = self.tweets['created_at'].dt.date\n",
        "\n",
        "            # Group by date and calculate metrics\n",
        "            tweet_sentiment = self.tweets.groupby('date').agg({\n",
        "                'sentiment': ['mean', 'std', 'count'],\n",
        "                'retweet_count': 'sum',\n",
        "                'favorite_count': 'sum'\n",
        "            })\n",
        "\n",
        "            tweet_sentiment.columns = ['tweet_sentiment_mean', 'tweet_sentiment_std',\n",
        "                                     'tweet_count', 'retweet_count', 'favorite_count']\n",
        "\n",
        "            # Convert index to datetime for proper joining\n",
        "            tweet_sentiment.index = pd.to_datetime(tweet_sentiment.index)\n",
        "\n",
        "            # Join with main sentiment DataFrame\n",
        "            sentiment_daily = sentiment_daily.join(tweet_sentiment)\n",
        "\n",
        "        # Process news sentiment\n",
        "        if self.news is not None and not self.news.empty:\n",
        "            # Convert to datetime and extract date\n",
        "            self.news['date'] = self.news['published_date'].dt.date\n",
        "\n",
        "            # Group by date and calculate metrics\n",
        "            news_sentiment = self.news.groupby('date').agg({\n",
        "                'sentiment': ['mean', 'std', 'count']\n",
        "            })\n",
        "\n",
        "            news_sentiment.columns = ['news_sentiment_mean', 'news_sentiment_std', 'news_count']\n",
        "\n",
        "            # Convert index to datetime for proper joining\n",
        "            news_sentiment.index = pd.to_datetime(news_sentiment.index)\n",
        "\n",
        "            # Join with main sentiment DataFrame\n",
        "            sentiment_daily = sentiment_daily.join(news_sentiment)\n",
        "\n",
        "        # Fill NaN values with 0 for calculation purposes\n",
        "        sentiment_daily = sentiment_daily.fillna(0)\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        if 'tweet_sentiment_mean' in sentiment_daily.columns:\n",
        "            sentiment_daily['tweet_sentiment_zscore'] = (\n",
        "                sentiment_daily['tweet_sentiment_mean'] -\n",
        "                sentiment_daily['tweet_sentiment_mean'].rolling(window=5).mean()\n",
        "            ) / sentiment_daily['tweet_sentiment_mean'].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        if 'news_sentiment_mean' in sentiment_daily.columns:\n",
        "            sentiment_daily['news_sentiment_zscore'] = (\n",
        "                sentiment_daily['news_sentiment_mean'] -\n",
        "                sentiment_daily['news_sentiment_mean'].rolling(window=5).mean()\n",
        "            ) / sentiment_daily['news_sentiment_mean'].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        # Add sentiment momentum (change from previous day)\n",
        "        for col in ['tweet_sentiment_mean', 'news_sentiment_mean']:\n",
        "            if col in sentiment_daily.columns:\n",
        "                sentiment_daily[f'{col}_change'] = sentiment_daily[col].diff()\n",
        "\n",
        "        # Add volume change metrics\n",
        "        for col in ['tweet_count', 'news_count']:\n",
        "            if col in sentiment_daily.columns:\n",
        "                sentiment_daily[f'{col}_change'] = sentiment_daily[col].pct_change()\n",
        "                sentiment_daily[f'{col}_zscore'] = (\n",
        "                    sentiment_daily[col] -\n",
        "                    sentiment_daily[col].rolling(window=5).mean()\n",
        "                ) / sentiment_daily[col].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        return sentiment_daily\n",
        "\n",
        "    def integrate_data(self):\n",
        "        \"\"\"Integrate stock data with sentiment analysis\"\"\"\n",
        "        print(\"Integrating market and sentiment data...\")\n",
        "\n",
        "        # Get sentiment data\n",
        "        sentiment_daily = self.analyze_sentiment()\n",
        "\n",
        "        # Make sure stock_data index is datetime\n",
        "        self.stock_data.index = pd.to_datetime(self.stock_data.index)\n",
        "\n",
        "        # Merge sentiment with stock data\n",
        "        merged_data = self.stock_data.join(sentiment_daily, how='left')\n",
        "\n",
        "        # Fill missing values\n",
        "        merged_data = merged_data.fillna(0)\n",
        "\n",
        "        # Calculate correlations between sentiment and price/volume changes\n",
        "        # These correlations can help identify manipulation\n",
        "        if 'tweet_sentiment_mean' in merged_data.columns:\n",
        "            merged_data['tweet_price_corr'] = merged_data['tweet_sentiment_mean'].rolling(window=5).corr(merged_data['price_change'])\n",
        "            merged_data['tweet_volume_corr'] = merged_data['tweet_sentiment_mean'].rolling(window=5).corr(merged_data['volume_change'])\n",
        "\n",
        "        if 'news_sentiment_mean' in merged_data.columns:\n",
        "            merged_data['news_price_corr'] = merged_data['news_sentiment_mean'].rolling(window=5).corr(merged_data['price_change'])\n",
        "            merged_data['news_volume_corr'] = merged_data['news_sentiment_mean'].rolling(window=5).corr(merged_data['volume_change'])\n",
        "\n",
        "        # Add features that might indicate manipulation\n",
        "        # 1. Abnormal price changes with high sentiment but low news (pump)\n",
        "        if 'tweet_count' in merged_data.columns and 'news_count' in merged_data.columns:\n",
        "            merged_data['pump_indicator'] = (\n",
        "                (merged_data['price_z_score'] > 1.5) &\n",
        "                (merged_data['tweet_sentiment_zscore'] > 1.5) &\n",
        "                (merged_data['news_count'] < merged_data['news_count'].mean())\n",
        "            ).astype(int)\n",
        "\n",
        "        # 2. High volume with negative sentiment divergence (dump)\n",
        "        if 'tweet_sentiment_zscore' in merged_data.columns:\n",
        "            merged_data['dump_indicator'] = (\n",
        "                (merged_data['volume_z_score'] > 1.5) &\n",
        "                (merged_data['price_change'] < 0) &\n",
        "                (merged_data['tweet_sentiment_zscore'] < -1.5)\n",
        "            ).astype(int)\n",
        "\n",
        "        # Keep only the most recent lookback days\n",
        "        merged_data = merged_data.iloc[-self.lookback_days:]\n",
        "\n",
        "        return merged_data\n",
        "\n",
        "    def train_anomaly_model(self, data):\n",
        "        \"\"\"Train isolation forest model for anomaly detection\"\"\"\n",
        "        print(\"Training anomaly detection model...\")\n",
        "\n",
        "        # Select features for anomaly detection\n",
        "        feature_cols = [\n",
        "            'price_z_score', 'volume_z_score', 'volatility',\n",
        "            'high_low_diff', 'price_momentum', 'volume_momentum'\n",
        "        ]\n",
        "\n",
        "        # Add sentiment features if available\n",
        "        sentiment_features = [\n",
        "            'tweet_sentiment_zscore', 'news_sentiment_zscore',\n",
        "            'tweet_count_zscore', 'news_count_zscore',\n",
        "            'tweet_price_corr', 'news_price_corr'\n",
        "        ]\n",
        "\n",
        "        for feature in sentiment_features:\n",
        "            if feature in data.columns:\n",
        "                feature_cols.append(feature)\n",
        "\n",
        "        # Get feature subset that exists in the data\n",
        "        valid_features = [col for col in feature_cols if col in data.columns]\n",
        "\n",
        "        if not valid_features:\n",
        "            print(\"No valid features found for anomaly detection\")\n",
        "            return False\n",
        "\n",
        "        # Extract features\n",
        "        X = data[valid_features].fillna(0)\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Train isolation forest\n",
        "        self.anomaly_model = IsolationForest(\n",
        "            n_estimators=100,\n",
        "            contamination=0.05,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        self.anomaly_model.fit(X_scaled)\n",
        "\n",
        "        # Add anomaly scores to the data\n",
        "        data['anomaly_score'] = self.anomaly_model.decision_function(X_scaled)\n",
        "        data['is_anomaly'] = self.anomaly_model.predict(X_scaled)\n",
        "\n",
        "        # Convert prediction to binary (1 for normal, -1 for anomaly)\n",
        "        data['is_anomaly'] = (data['is_anomaly'] == -1).astype(int)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def train_manipulation_model(self, data):\n",
        "        \"\"\"Train XGBoost model to classify potential manipulation\"\"\"\n",
        "        print(\"Training manipulation detection model...\")\n",
        "\n",
        "        # For a real system, you would have labeled data of known manipulation cases\n",
        "        # Since we don't have labels, we'll create synthetic ones based on our indicators\n",
        "        # ... existing code ...\n",
        "\n",
        "        # Define manipulation as days with anomalies and either pump or dump indicators\n",
        "        if 'pump_indicator' in data.columns and 'dump_indicator' in data.columns:\n",
        "            data['potential_manipulation'] = (\n",
        "                (data['is_anomaly'] == 1) &\n",
        "                ((data['pump_indicator'] == 1) | (data['dump_indicator'] == 1))\n",
        "            ).astype(int)\n",
        "        else:\n",
        "            # Fallback to just anomalies if we don't have the indicators\n",
        "            data['potential_manipulation'] = data['is_anomaly']\n",
        "\n",
        "        # Select features for the classifier\n",
        "        feature_cols = [\n",
        "            'price_z_score', 'volume_z_score', 'volatility',\n",
        "            'high_low_diff', 'price_momentum', 'volume_momentum'\n",
        "        ]\n",
        "\n",
        "        # Add sentiment features if available\n",
        "        sentiment_features = [\n",
        "            'tweet_sentiment_mean', 'news_sentiment_mean',\n",
        "            'tweet_count', 'news_count',\n",
        "            'tweet_sentiment_zscore', 'news_sentiment_zscore',\n",
        "            'tweet_price_corr', 'news_price_corr'\n",
        "        ]\n",
        "\n",
        "        for feature in sentiment_features:\n",
        "            if feature in data.columns:\n",
        "                feature_cols.append(feature)\n",
        "\n",
        "        # Get feature subset that exists in the data\n",
        "        valid_features = [col for col in feature_cols if col in data.columns]\n",
        "\n",
        "        if not valid_features:\n",
        "            print(\"No valid features found for manipulation model\")\n",
        "            return data\n",
        "\n",
        "        # Extract features and target\n",
        "        X = data[valid_features].fillna(0)\n",
        "        y = data['potential_manipulation']\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Train XGBoost model\n",
        "        self.manipulation_model = xgb.XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=3,\n",
        "            random_state=42,\n",
        "            use_label_encoder=False,\n",
        "            eval_metric='logloss'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Only train if we have both classes represented\n",
        "            if len(y.unique()) > 1:\n",
        "                # Split data into train and test sets\n",
        "                from sklearn.model_selection import train_test_split\n",
        "                X_train, X_test, y_train, y_test = train_test_split(\n",
        "                    X_scaled, y, test_size=0.2, random_state=42\n",
        "                )\n",
        "\n",
        "                # Train model\n",
        "                self.manipulation_model.fit(X_train, y_train)\n",
        "\n",
        "                # Make predictions on test set\n",
        "                y_pred = self.manipulation_model.predict(X_test)\n",
        "                y_pred_proba = self.manipulation_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "                # Calculate metrics\n",
        "                precision = precision_score(y_test, y_pred)\n",
        "                recall = recall_score(y_test, y_pred)\n",
        "                f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "                print(\"\\n----- Model Performance Metrics -----\")\n",
        "                print(f\"Precision: {precision:.3f}\")\n",
        "                print(f\"Recall: {recall:.3f}\")\n",
        "                print(f\"F1 Score: {f1:.3f}\")\n",
        "\n",
        "                # Add predictions to the data\n",
        "                data['manipulation_probability'] = self.manipulation_model.predict_proba(X_scaled)[:, 1]\n",
        "                data['predicted_manipulation'] = self.manipulation_model.predict(X_scaled)\n",
        "\n",
        "                # Feature importance\n",
        "                feature_importance = pd.DataFrame({\n",
        "                    'feature': valid_features,\n",
        "                    'importance': self.manipulation_model.feature_importances_\n",
        "                }).sort_values('importance', ascending=False)\n",
        "\n",
        "                print(\"\\nTop manipulation indicators:\")\n",
        "                print(feature_importance.head(5))\n",
        "\n",
        "                # Plot ROC curve\n",
        "                from sklearn.metrics import roc_curve, auc\n",
        "                fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "                roc_auc = auc(fpr, tpr)\n",
        "\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "                        label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "                plt.xlim([0.0, 1.0])\n",
        "                plt.ylim([0.0, 1.05])\n",
        "                plt.xlabel('False Positive Rate')\n",
        "                plt.ylabel('True Positive Rate')\n",
        "                plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "                plt.legend(loc=\"lower right\")\n",
        "                plt.show()\n",
        "\n",
        "            else:\n",
        "                print(\"Not enough variation in the target variable to train classifier\")\n",
        "                data['manipulation_probability'] = 0\n",
        "                data['predicted_manipulation'] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training manipulation model: {e}\")\n",
        "            data['manipulation_probability'] = 0\n",
        "            data['predicted_manipulation'] = 0\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def detect_manipulation(self, demo_mode=True):\n",
        "        \"\"\"Main method to run the entire detection pipeline\"\"\"\n",
        "        # Fetch data\n",
        "        if not self.fetch_stock_data():\n",
        "            print(\"Failed to fetch stock data. Aborting.\")\n",
        "            return None\n",
        "\n",
        "        if demo_mode:\n",
        "            # Use alternative data sources that don't require APIs\n",
        "            self.fetch_tweets_alternative(self.ticker)\n",
        "            self.fetch_news_alternative(self.ticker)\n",
        "        else:\n",
        "            # Try to use actual APIs (may fail with current limitations)\n",
        "            self.fetch_tweets()\n",
        "            self.fetch_news()\n",
        "\n",
        "    # Rest of the method remains the same...\n",
        "\n",
        "    def display_results(self, data):\n",
        "        \"\"\"Display detection results and visualizations\"\"\"\n",
        "        if data is None or data.empty:\n",
        "            print(\"No data available to display results\")\n",
        "            return\n",
        "\n",
        "        # Print summary of detected manipulations\n",
        "        print(\"\\n----- MANIPULATION DETECTION SUMMARY -----\")\n",
        "\n",
        "        # Filter to just the most recent period\n",
        "        recent_data = data.iloc[-self.lookback_days:]\n",
        "\n",
        "        # Count days with potential manipulation\n",
        "        if 'predicted_manipulation' in recent_data.columns:\n",
        "            manipulation_days = recent_data[recent_data['predicted_manipulation'] == 1]\n",
        "            n_manipulation_days = len(manipulation_days)\n",
        "\n",
        "            print(f\"Detected potential manipulation on {n_manipulation_days} days out of {len(recent_data)} analyzed.\")\n",
        "\n",
        "            if n_manipulation_days > 0:\n",
        "                print(\"\\nDates with suspected manipulation:\")\n",
        "                for date, row in manipulation_days.iterrows():\n",
        "                    features = []\n",
        "\n",
        "                    # Add indicators that triggered the alert\n",
        "                    if row['price_z_score'] > 1.5:\n",
        "                        features.append(f\"Abnormal price (z={row['price_z_score']:.2f})\")\n",
        "                    if row['volume_z_score'] > 1.5:\n",
        "                        features.append(f\"Abnormal volume (z={row['volume_z_score']:.2f})\")\n",
        "                    if 'tweet_sentiment_zscore' in row and row['tweet_sentiment_zscore'] > 1.5:\n",
        "                        features.append(f\"Abnormal social sentiment (z={row['tweet_sentiment_zscore']:.2f})\")\n",
        "                    if 'pump_indicator' in row and row['pump_indicator'] == 1:\n",
        "                        features.append(\"Pump pattern\")\n",
        "                    if 'dump_indicator' in row and row['dump_indicator'] == 1:\n",
        "                        features.append(\"Dump pattern\")\n",
        "\n",
        "                    print(f\"  {date.date()}: {', '.join(features)}\")\n",
        "        else:\n",
        "            print(\"Manipulation classification not available.\")\n",
        "\n",
        "        # Plot results\n",
        "        try:\n",
        "            self.plot_results(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating plots: {e}\")\n",
        "\n",
        "    def plot_results(self, data):\n",
        "        \"\"\"Create visualizations of the detection results\"\"\"\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(14, 18), sharex=True)\n",
        "\n",
        "        # Stock price with anomaly highlighting\n",
        "        ax0 = axes[0]\n",
        "        ax0.set_title(f\"{self.ticker} Stock Price with Anomaly Detection\", fontsize=14)\n",
        "        ax0.plot(data.index, data['close'], label='Close Price', color='blue')\n",
        "\n",
        "        # Highlight anomalies if available\n",
        "        if 'is_anomaly' in data.columns:\n",
        "            anomaly_days = data[data['is_anomaly'] == 1]\n",
        "            ax0.scatter(anomaly_days.index, anomaly_days['close'],\n",
        "                      color='red', label='Anomalies', zorder=5)\n",
        "\n",
        "        # Highlight manipulation if available\n",
        "        if 'predicted_manipulation' in data.columns:\n",
        "            manip_days = data[data['predicted_manipulation'] == 1]\n",
        "            ax0.scatter(manip_days.index, manip_days['close'],\n",
        "                      color='darkred', marker='X', s=100,\n",
        "                      label='Potential Manipulation', zorder=10)\n",
        "\n",
        "        ax0.set_ylabel('Price ($)')\n",
        "        ax0.legend()\n",
        "        ax0.grid(True, alpha=0.3)\n",
        "\n",
        "        # Volume plot\n",
        "        ax1 = axes[1]\n",
        "        ax1.set_title(f\"{self.ticker} Trading Volume\", fontsize=14)\n",
        "        ax1.bar(data.index, data['volume'], color='green', alpha=0.7, label='Volume')\n",
        "\n",
        "        # Highlight volume anomalies\n",
        "        volume_anomalies = data[data['volume_z_score'] > 1.5]\n",
        "        ax1.bar(volume_anomalies.index, volume_anomalies['volume'], color='orange', label='Volume Anomalies')\n",
        "\n",
        "        ax1.set_ylabel('Volume')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Sentiment plot\n",
        "        ax2 = axes[2]\n",
        "        ax2.set_title(\"Sentiment Analysis\", fontsize=14)\n",
        "\n",
        "        if 'tweet_sentiment_mean' in data.columns:\n",
        "            ax2.plot(data.index, data['tweet_sentiment_mean'],\n",
        "                   label='Social Sentiment', color='purple')\n",
        "\n",
        "        if 'news_sentiment_mean' in data.columns:\n",
        "            ax2.plot(data.index, data['news_sentiment_mean'],\n",
        "                   label='News Sentiment', color='brown')\n",
        "\n",
        "        # Add zero line\n",
        "        ax2.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
        "\n",
        "        ax2.set_ylabel('Sentiment Score')\n",
        "        ax2.set_xlabel('Date')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.ticker}_manipulation_analysis.png\")\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"\\nSaved visualization to {self.ticker}_manipulation_analysis.png\")\n",
        "\n",
        "def test_detector(ticker_symbol, demo_mode=True):\n",
        "    print(f\"\\n===== ANALYZING {ticker_symbol} =====\")\n",
        "    detector = StockManipulationDetector(ticker_symbol, lookback_days=30)\n",
        "    results = detector.detect_manipulation(demo_mode=demo_mode)\n",
        "    return results\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Test with a few stocks known for volatility and social media attention\n",
        "    tickers = [\"GME\", \"AMC\", \"TSLA\", \"AAPL\"]\n",
        "\n",
        "    for ticker in tickers:\n",
        "        test_detector(ticker)\n",
        "        # Add delay between API calls to avoid rate limits\n",
        "        time.sleep(2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UURdO2cPms0F",
        "outputId": "891b83c1-9876-4f6a-85ab-bfa8bceae5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ANALYZING GME =====\n",
            "Fetching stock data for GME...\n",
            "Error fetching stock data. API response: {'Information': 'We have detected your API key as PIG3WPABVKTBMH6Y and our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}\n",
            "Failed to fetch stock data. Aborting.\n",
            "\n",
            "===== ANALYZING AMC =====\n",
            "Fetching stock data for AMC...\n",
            "Error fetching stock data. API response: {'Information': 'We have detected your API key as PIG3WPABVKTBMH6Y and our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}\n",
            "Failed to fetch stock data. Aborting.\n",
            "\n",
            "===== ANALYZING TSLA =====\n",
            "Fetching stock data for TSLA...\n",
            "Error fetching stock data. API response: {'Information': 'We have detected your API key as PIG3WPABVKTBMH6Y and our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}\n",
            "Failed to fetch stock data. Aborting.\n",
            "\n",
            "===== ANALYZING AAPL =====\n",
            "Fetching stock data for AAPL...\n",
            "Error fetching stock data. API response: {'Information': 'We have detected your API key as PIG3WPABVKTBMH6Y and our standard API rate limit is 25 requests per day. Please subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly remove all daily rate limits.'}\n",
            "Failed to fetch stock data. Aborting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import tweepy\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# installing sentiment analyzer model\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "# initializing this instance\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Historical Stock Data Setup\n",
        "ALPHA_VANTAGE_API_KEY = \"PIG3WPABVKTBMH6Y\"\n",
        "\n",
        "# Twitter API setup\n",
        "TWITTER_API_KEY = \"ZkBtakhypMnFkI4dUzVo0QJTw\"\n",
        "TWITTER_API_SECRET = \"P1gqWUJsOFkjmkOsPuyB458xi8bwo4KZ1Cy0LbGXPQLxcR3v79\"\n",
        "TWITTER_ACCESS_TOKEN = \"1916142259252432899-t6yhBBktXrrexsMqb0DOR9ZgVDJJTB\"\n",
        "TWITTER_ACCESS_SECRET = \"L29j1XSBBYhJ614Ev2cE7Ukl3a0Vs5iwMxC9iyOEOJ3oR\"\n",
        "\n",
        "# Authenticate with Twitter API\n",
        "auth = tweepy.OAuthHandler(TWITTER_API_KEY, TWITTER_API_SECRET)\n",
        "auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_SECRET)\n",
        "twitter_api = tweepy.API(auth)\n",
        "\n",
        "# Class for stock manipulation detection\n",
        "class StockManipulationDetector:\n",
        "    def __init__(self, ticker_symbol, lookback_days=30):\n",
        "        self.ticker = ticker_symbol\n",
        "        self.lookback_days = lookback_days\n",
        "        self.stock_data = None\n",
        "        self.tweets = None\n",
        "        self.news = None\n",
        "        self.anomaly_model = None\n",
        "        self.manipulation_model = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def fetch_stock_data(self):\n",
        "        \"\"\"Fetch historical stock data from Alpha Vantage\"\"\"\n",
        "        print(f\"Fetching stock data for {self.ticker}...\")\n",
        "\n",
        "        # Daily data\n",
        "        url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={self.ticker}&outputsize=full&apikey={ALPHA_VANTAGE_API_KEY}\"\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        if \"Time Series (Daily)\" not in data:\n",
        "            print(\"Error fetching stock data. API response:\", data)\n",
        "            return False\n",
        "\n",
        "        df = pd.DataFrame(data[\"Time Series (Daily)\"]).T\n",
        "        df.columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "        df = df.astype(float)\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "        df = df.sort_index()\n",
        "\n",
        "        # Calculate additional features\n",
        "        df['price_change'] = df['close'].pct_change()\n",
        "        df['volume_change'] = df['volume'].pct_change()\n",
        "        df['high_low_diff'] = df['high'] - df['low']\n",
        "        df['volatility'] = df['price_change'].rolling(window=5).std()\n",
        "\n",
        "        # Rolling statistics\n",
        "        df['price_mean_5d'] = df['close'].rolling(window=5).mean()\n",
        "        df['volume_mean_5d'] = df['volume'].rolling(window=5).mean()\n",
        "        df['price_std_5d'] = df['close'].rolling(window=5).std()\n",
        "        df['volume_std_5d'] = df['volume'].rolling(window=5).std()\n",
        "\n",
        "        # Z-scores for anomaly detection\n",
        "        df['price_z_score'] = (df['close'] - df['price_mean_5d']) / df['price_std_5d']\n",
        "        df['volume_z_score'] = (df['volume'] - df['volume_mean_5d']) / df['volume_std_5d']\n",
        "\n",
        "        # Momentum indicators\n",
        "        df['price_momentum'] = df['close'] - df['close'].shift(5)\n",
        "        df['volume_momentum'] = df['volume'] - df['volume'].shift(5)\n",
        "\n",
        "        # Filter to relevant period and drop NAs\n",
        "        df = df.iloc[-self.lookback_days*2:]\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        self.stock_data = df\n",
        "        print(f\"Fetched {len(df)} days of stock data\")\n",
        "        return True\n",
        "\n",
        "    # Simplified Twitter sentiment collection using text files or CSV instead of API\n",
        "    def fetch_tweets_alternative(self, ticker):\n",
        "        \"\"\"Alternative method when Twitter API is unavailable\"\"\"\n",
        "        print(f\"Using alternative sentiment data for {ticker}...\")\n",
        "\n",
        "        # Create synthetic sentiment data based on stock price movements\n",
        "        # This is a fallback when real Twitter data isn't available\n",
        "        if self.stock_data is not None:\n",
        "            dates = self.stock_data.index[-30:]  # Last 30 days\n",
        "\n",
        "            # Create synthetic tweet sentiment that somewhat follows price changes\n",
        "            # but with some randomness and lag\n",
        "            price_changes = self.stock_data['price_change'].values[-32:-2]  # Lagged by 2 days\n",
        "\n",
        "            synthetic_tweet_data = []\n",
        "\n",
        "            for i, date in enumerate(dates):\n",
        "                # Base sentiment on lagged price changes with noise\n",
        "                base_sentiment = price_changes[i] * 5  # Scale up for sentiment range\n",
        "                sentiment = min(max(base_sentiment + np.random.normal(0, 0.3), -1), 1)  # Bound between -1 and 1\n",
        "\n",
        "                # Create more tweets on volatile days\n",
        "                tweet_count = int(50 + abs(sentiment) * 200 + np.random.normal(0, 20))\n",
        "                tweet_count = max(10, tweet_count)  # At least 10 tweets\n",
        "\n",
        "                synthetic_tweet_data.append({\n",
        "                    'date': date,\n",
        "                    'tweet_sentiment_mean': sentiment,\n",
        "                    'tweet_sentiment_std': 0.3 + abs(sentiment) * 0.2,\n",
        "                    'tweet_count': tweet_count,\n",
        "                    'retweet_count': tweet_count * 3,\n",
        "                    'favorite_count': tweet_count * 5\n",
        "                })\n",
        "\n",
        "            self.tweets = pd.DataFrame(synthetic_tweet_data)\n",
        "            self.tweets.set_index('date', inplace=True)\n",
        "            print(f\"Created synthetic sentiment data for {ticker}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    # Simplified news collection using text files or CSV instead of web scraping\n",
        "    def fetch_news_alternative(self, ticker):\n",
        "        \"\"\"Alternative method when news scraping is blocked\"\"\"\n",
        "        print(f\"Using alternative news data for {ticker}...\")\n",
        "\n",
        "        # Create synthetic news data based on stock movements\n",
        "        if self.stock_data is not None:\n",
        "            dates = self.stock_data.index[-30:]  # Last 30 days\n",
        "\n",
        "            # Create news sentiment that somewhat follows price trends\n",
        "            # but with occasional contrarian articles\n",
        "            synthetic_news_data = []\n",
        "\n",
        "            for date in dates:\n",
        "                # Get price data for this date if available\n",
        "                if date in self.stock_data.index:\n",
        "                    price_change = self.stock_data.loc[date, 'price_change']\n",
        "\n",
        "                    # Occasionally have contrarian news\n",
        "                    contrarian = np.random.random() > 0.7\n",
        "\n",
        "                    if contrarian:\n",
        "                        # News sentiment opposite to price movement\n",
        "                        sentiment = -price_change * 3\n",
        "                    else:\n",
        "                        # News sentiment aligned with price movement\n",
        "                        sentiment = price_change * 3\n",
        "\n",
        "                    sentiment = min(max(sentiment + np.random.normal(0, 0.2), -1), 1)\n",
        "\n",
        "                    # More news on days with bigger price moves\n",
        "                    news_count = int(2 + abs(price_change) * 20 + np.random.normal(0, 1))\n",
        "                    news_count = max(1, news_count)  # At least 1 news item\n",
        "\n",
        "                    synthetic_news_data.append({\n",
        "                        'date': date,\n",
        "                        'news_sentiment_mean': sentiment,\n",
        "                        'news_sentiment_std': 0.2,\n",
        "                        'news_count': news_count\n",
        "                    })\n",
        "\n",
        "            self.news = pd.DataFrame(synthetic_news_data)\n",
        "            self.news.set_index('date', inplace=True)\n",
        "            print(f\"Created synthetic news data for {ticker}\")\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def analyze_sentiment(self):\n",
        "        \"\"\"Analyze 7sentiment from tweets and news, aggregate by day\"\"\"\n",
        "        print(\"Analyzing sentiment data...\")\n",
        "\n",
        "        # Create date ranges for the period we're analyzing\n",
        "        end_date = datetime.now().date()\n",
        "        start_date = end_date - timedelta(days=self.lookback_days)\n",
        "        date_range = pd.date_range(start=start_date, end=end_date)\n",
        "\n",
        "        # Initialize sentiment DataFrames\n",
        "        sentiment_daily = pd.DataFrame(index=date_range)\n",
        "\n",
        "        # Process tweets sentiment\n",
        "        if self.tweets is not None and not self.tweets.empty:\n",
        "            # The 'date' column is already present in self.tweets DataFrame\n",
        "            # and is used as the index.\n",
        "            # self.tweets['date'] = self.tweets['created_at'].dt.date\n",
        "\n",
        "            # Group by date and calculate metrics\n",
        "            tweet_sentiment = self.tweets.groupby(self.tweets.index).agg({\n",
        "                'tweet_sentiment_mean': 'mean',\n",
        "                'tweet_sentiment_std': 'std',\n",
        "                'tweet_count': 'sum',\n",
        "                'retweet_count': 'sum',\n",
        "                'favorite_count': 'sum'\n",
        "            })\n",
        "\n",
        "            # tweet_sentiment.columns = ['tweet_sentiment_mean', 'tweet_sentiment_std',\n",
        "            #                          'tweet_count', 'retweet_count', 'favorite_count']\n",
        "\n",
        "            # Convert index to datetime for proper joining\n",
        "            # tweet_sentiment.index = pd.to_datetime(tweet_sentiment.index)\n",
        "\n",
        "            # Join with main sentiment DataFrame\n",
        "            sentiment_daily = sentiment_daily.join(tweet_sentiment)\n",
        "\n",
        "        # Process news sentiment\n",
        "        if self.news is not None and not self.news.empty:\n",
        "            # Convert to datetime and extract date\n",
        "            # The news dataframe already has a 'date' column,\n",
        "            # just converting to datetime\n",
        "            self.news = self.news.reset_index()\n",
        "            self.news['date'] = pd.to_datetime(self.news['date']).dt.date\n",
        "            #self.news['date'] = self.news['published_date'].dt.date\n",
        "\n",
        "            # Group by date and calculate metrics\n",
        "            news_sentiment = self.news.groupby('date').agg({\n",
        "                'news_sentiment_mean': ['mean', 'std'], #changed 'sentiment' to actual columns\n",
        "                'news_count': 'sum'                      #changed 'sentiment' to actual columns\n",
        "            })\n",
        "\n",
        "            news_sentiment.columns = ['news_sentiment_mean', 'news_sentiment_std', 'news_count']\n",
        "\n",
        "            # Convert index to datetime for proper joining\n",
        "            news_sentiment.index = pd.to_datetime(news_sentiment.index)\n",
        "\n",
        "            # Join with main sentiment DataFrame\n",
        "            sentiment_daily = sentiment_daily.join(news_sentiment)\n",
        "\n",
        "        # Fill NaN values with 0 for calculation purposes\n",
        "        sentiment_daily = sentiment_daily.fillna(0)\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        if 'tweet_sentiment_mean' in sentiment_daily.columns:\n",
        "            sentiment_daily['tweet_sentiment_zscore'] = (\n",
        "                sentiment_daily['tweet_sentiment_mean'] -\n",
        "                sentiment_daily['tweet_sentiment_mean'].rolling(window=5).mean()\n",
        "            ) / sentiment_daily['tweet_sentiment_mean'].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        if 'news_sentiment_mean' in sentiment_daily.columns:\n",
        "            sentiment_daily['news_sentiment_zscore'] = (\n",
        "                sentiment_daily['news_sentiment_mean'] -\n",
        "                sentiment_daily['news_sentiment_mean'].rolling(window=5).mean()\n",
        "            ) / sentiment_daily['news_sentiment_mean'].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        # Add sentiment momentum (change from previous day)\n",
        "        for col in ['tweet_sentiment_mean', 'news_sentiment_mean']:\n",
        "            if col in sentiment_daily.columns:\n",
        "                sentiment_daily[f'{col}_change'] = sentiment_daily[col].diff()\n",
        "\n",
        "        # Add volume change metrics\n",
        "        for col in ['tweet_count', 'news_count']:\n",
        "            if col in sentiment_daily.columns:\n",
        "                sentiment_daily[f'{col}_change'] = sentiment_daily[col].pct_change()\n",
        "                sentiment_daily[f'{col}_zscore'] = (\n",
        "                    sentiment_daily[col] -\n",
        "                    sentiment_daily[col].rolling(window=5).mean()\n",
        "                ) / sentiment_daily[col].rolling(window=5).std().replace(0, 1)\n",
        "\n",
        "        return sentiment_daily\n",
        "\n",
        "    def integrate_data(self):\n",
        "        \"\"\"Integrate stock data with sentiment analysis\"\"\"\n",
        "        print(\"Integrating market and sentiment data...\")\n",
        "\n",
        "        sentiment_daily = self.analyze_sentiment()\n",
        "\n",
        "        self.stock_data.index = pd.to_datetime(self.stock_data.index)\n",
        "\n",
        "        merged_data = self.stock_data.join(sentiment_daily, how='left')\n",
        "\n",
        "        merged_data = merged_data.fillna(0)\n",
        "\n",
        "        if 'tweet_sentiment_mean' in merged_data.columns:\n",
        "            merged_data['tweet_price_corr'] = merged_data['tweet_sentiment_mean'].rolling(window=5).corr(merged_data['price_change'])\n",
        "            merged_data['tweet_volume_corr'] = merged_data['tweet_sentiment_mean'].rolling(window=5).corr(merged_data['volume_change'])\n",
        "\n",
        "        if 'news_sentiment_mean' in merged_data.columns:\n",
        "            merged_data['news_price_corr'] = merged_data['news_sentiment_mean'].rolling(window=5).corr(merged_data['price_change'])\n",
        "            merged_data['news_volume_corr'] = merged_data['news_sentiment_mean'].rolling(window=5).corr(merged_data['volume_change'])\n",
        "\n",
        "        if 'tweet_count' in merged_data.columns and 'news_count' in merged_data.columns:\n",
        "            merged_data['pump_indicator'] = (\n",
        "                (merged_data['price_z_score'] > 1.0) &\n",
        "                (merged_data['tweet_sentiment_zscore'] > 1.0) &\n",
        "                (merged_data['news_count'] < merged_data['news_count'].mean())\n",
        "            ).astype(int)\n",
        "\n",
        "        if 'tweet_sentiment_zscore' in merged_data.columns:\n",
        "            merged_data['dump_indicator'] = (\n",
        "                (merged_data['volume_z_score'] > 1.0) &\n",
        "                (merged_data['price_change'] < 0) &\n",
        "                (merged_data['tweet_sentiment_zscore'] < -1.0)\n",
        "            ).astype(int)\n",
        "\n",
        "        merged_data = merged_data.iloc[-self.lookback_days:]\n",
        "\n",
        "        return merged_data\n",
        "\n",
        "    def train_anomaly_model(self, data):\n",
        "        \"\"\"Train isolation forest model for anomaly detection\"\"\"\n",
        "        print(\"Training anomaly detection model...\")\n",
        "\n",
        "        feature_cols = [\n",
        "            'price_z_score', 'volume_z_score', 'volatility',\n",
        "            'high_low_diff', 'price_momentum', 'volume_momentum'\n",
        "        ]\n",
        "\n",
        "        sentiment_features = [\n",
        "            'tweet_sentiment_zscore', 'news_sentiment_zscore',\n",
        "            'tweet_count_zscore', 'news_count_zscore',\n",
        "            'tweet_price_corr', 'news_price_corr'\n",
        "        ]\n",
        "\n",
        "        for feature in sentiment_features:\n",
        "            if feature in data.columns:\n",
        "                feature_cols.append(feature)\n",
        "\n",
        "        valid_features = [col for col in feature_cols if col in data.columns]\n",
        "\n",
        "        if not valid_features:\n",
        "            print(\"No valid features found for anomaly detection\")\n",
        "            return False\n",
        "\n",
        "        X = data[valid_features].fillna(0)\n",
        "\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        self.anomaly_model = IsolationForest(\n",
        "            n_estimators=100,\n",
        "            contamination=0.05,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        self.anomaly_model.fit(X_scaled)\n",
        "\n",
        "        data['anomaly_score'] = self.anomaly_model.decision_function(X_scaled)\n",
        "        data['is_anomaly'] = self.anomaly_model.predict(X_scaled)\n",
        "\n",
        "        data['is_anomaly'] = (data['is_anomaly'] == -1).astype(int)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def train_manipulation_model(self, data):\n",
        "        \"\"\"Train XGBoost model to classify potential manipulation\"\"\"\n",
        "\n",
        "        print(\"Training manipulation detection model...\")\n",
        "        if 'pump_indicator' in data.columns and 'dump_indicator' in data.columns:\n",
        "            data['potential_manipulation'] = (\n",
        "                (data['is_anomaly'] == 1) &\n",
        "                ((data['pump_indicator'] == 1) | (data['dump_indicator'] == 1))\n",
        "            ).astype(int)\n",
        "        else:\n",
        "            data['potential_manipulation'] = data['is_anomaly']\n",
        "\n",
        "        feature_cols = [\n",
        "            'price_z_score', 'volume_z_score', 'volatility',\n",
        "            'high_low_diff', 'price_momentum', 'volume_momentum'\n",
        "        ]\n",
        "\n",
        "        sentiment_features = [\n",
        "            'tweet_sentiment_mean', 'news_sentiment_mean',\n",
        "            'tweet_count', 'news_count',\n",
        "            'tweet_sentiment_zscore', 'news_sentiment_zscore',\n",
        "            'tweet_price_corr', 'news_price_corr'\n",
        "        ]\n",
        "\n",
        "        for feature in sentiment_features:\n",
        "            if feature in data.columns:\n",
        "                feature_cols.append(feature)\n",
        "\n",
        "        valid_features = [col for col in feature_cols if col in data.columns]\n",
        "\n",
        "        if not valid_features:\n",
        "            print(\"No valid features found for manipulation model\")\n",
        "            return data\n",
        "\n",
        "        X = data[valid_features].fillna(0)\n",
        "        y = data['potential_manipulation']\n",
        "\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        self.manipulation_model = xgb.XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=3,\n",
        "            random_state=42,\n",
        "            use_label_encoder=False,\n",
        "            eval_metric='logloss'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "           if len(y.unique()) > 1:\n",
        "               from sklearn.model_selection import train_test_split\n",
        "               X_train, X_test, y_train, y_test = train_test_split(\n",
        "                   X_scaled, y, test_size=0.2, random_state=42\n",
        "               )\n",
        "               self.manipulation_model.fit(X_train, y_train)\n",
        "               y_pred = self.manipulation_model.predict(X_test)\n",
        "               y_pred_proba = self.manipulation_model.predict_proba(X_test)[:, 1]\n",
        "               precision = precision_score(y_test, y_pred)\n",
        "               recall = recall_score(y_test, y_pred)\n",
        "               f1 = f1_score(y_test, y_pred)\n",
        "               print(\"\\n----- Model Performance Metrics -----\")\n",
        "               print(f\"Precision: {precision:.3f}\")\n",
        "               print(f\"Recall: {recall:.3f}\")\n",
        "               print(f\"F1 Score: {f1:.3f}\")\n",
        "\n",
        "               data['manipulation_probability'] = self.manipulation_model.predict_proba(X_scaled)[:, 1]\n",
        "               data['predicted_manipulation'] = self.manipulation_model.predict(X_scaled)\n",
        "\n",
        "               feature_importance = pd.DataFrame({\n",
        "                   'feature': valid_features,\n",
        "                   'importance': self.manipulation_model.feature_importances_\n",
        "               }).sort_values('importance', ascending=False)\n",
        "               print(\"\\nTop manipulation indicators:\")\n",
        "               print(feature_importance.head(5))\n",
        "\n",
        "               from sklearn.metrics import roc_curve, auc\n",
        "               fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "               roc_auc = auc(fpr, tpr)\n",
        "               plt.figure(figsize=(8, 6))\n",
        "               plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "                       label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "               plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "               plt.xlim([0.0, 1.0])\n",
        "               plt.ylim([0.0, 1.05])\n",
        "               plt.xlabel('False Positive Rate')\n",
        "               plt.ylabel('True Positive Rate')\n",
        "               plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "               plt.legend(loc=\"lower right\")\n",
        "               plt.show()\n",
        "           else:\n",
        "               print(\"Not enough variation in the target variable to train classifier\")\n",
        "               data['manipulation_probability'] = 0\n",
        "               data['predicted_manipulation'] = 0\n",
        "        except Exception as e:\n",
        "              print(f\"Error training manipulation model: {e}\")\n",
        "              data['manipulation_probability'] = 0\n",
        "              data['predicted_manipulation'] = 0\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def detect_manipulation(self, demo_mode=True):\n",
        "        \"\"\"Main method to run the entire detection pipeline\"\"\"\n",
        "        # Fetch data\n",
        "        if not self.fetch_stock_data():\n",
        "            print(\"Failed to fetch stock data. Aborting.\")\n",
        "            return None\n",
        "\n",
        "        if demo_mode:\n",
        "            self.fetch_tweets_alternative(self.ticker)\n",
        "            self.fetch_news_alternative(self.ticker)\n",
        "        else:\n",
        "            self.fetch_tweets()\n",
        "            self.fetch_news()\n",
        "\n",
        "        # Add these steps to complete the pipeline\n",
        "        # 1. Integrate all data sources\n",
        "        merged_data = self.integrate_data()\n",
        "\n",
        "        # 2. Train anomaly detection model\n",
        "        merged_data = self.train_anomaly_model(merged_data)\n",
        "\n",
        "        # 3. Train manipulation detection model\n",
        "        merged_data = self.train_manipulation_model(merged_data)\n",
        "\n",
        "        # 4. Display results\n",
        "        self.display_results(merged_data)\n",
        "\n",
        "        return merged_data\n",
        "\n",
        "    def display_results(self, data):\n",
        "        \"\"\"Display detection results and visualizations\"\"\"\n",
        "        if data is None or data.empty:\n",
        "            print(\"No data available to display results\")\n",
        "            return\n",
        "\n",
        "        # Print summary of detected manipulations\n",
        "        print(\"\\n----- MANIPULATION DETECTION SUMMARY -----\")\n",
        "\n",
        "        # Filter to just the most recent period\n",
        "        recent_data = data.iloc[-self.lookback_days:]\n",
        "\n",
        "        # Count days with potential manipulation\n",
        "        if 'predicted_manipulation' in recent_data.columns:\n",
        "            manipulation_days = recent_data[recent_data['predicted_manipulation'] == 1]\n",
        "            n_manipulation_days = len(manipulation_days)\n",
        "\n",
        "            print(f\"Detected potential manipulation on {n_manipulation_days} days out of {len(recent_data)} analyzed.\")\n",
        "\n",
        "            if n_manipulation_days > 0:\n",
        "                print(\"\\nDates with suspected manipulation:\")\n",
        "                for date, row in manipulation_days.iterrows():\n",
        "                    features = []\n",
        "\n",
        "                    # Add indicators that triggered the alert\n",
        "                    if row['price_z_score'] > 1.5:\n",
        "                        features.append(f\"Abnormal price (z={row['price_z_score']:.2f})\")\n",
        "                    if row['volume_z_score'] > 1.5:\n",
        "                        features.append(f\"Abnormal volume (z={row['volume_z_score']:.2f})\")\n",
        "                    if 'tweet_sentiment_zscore' in row and row['tweet_sentiment_zscore'] > 1.5:\n",
        "                        features.append(f\"Abnormal social sentiment (z={row['tweet_sentiment_zscore']:.2f})\")\n",
        "                    if 'pump_indicator' in row and row['pump_indicator'] == 1:\n",
        "                        features.append(\"Pump pattern\")\n",
        "                    if 'dump_indicator' in row and row['dump_indicator'] == 1:\n",
        "                        features.append(\"Dump pattern\")\n",
        "\n",
        "                    print(f\"  {date.date()}: {', '.join(features)}\")\n",
        "        else:\n",
        "            print(\"Manipulation classification not available.\")\n",
        "\n",
        "        # Plot results\n",
        "            try:\n",
        "                self.plot_results(data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error creating plots: {e}\")\n",
        "\n",
        "    def plot_results(self, data):\n",
        "        \"\"\"Create visualizations of the detection results\"\"\"\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(14, 18), sharex=True)\n",
        "\n",
        "        # Stock price with anomaly highlighting\n",
        "        ax0 = axes[0]\n",
        "        ax0.set_title(f\"{self.ticker} Stock Price with Anomaly Detection\", fontsize=14)\n",
        "        ax0.plot(data.index, data['close'], label='Close Price', color='blue')\n",
        "\n",
        "        # Highlight anomalies if available\n",
        "        if 'is_anomaly' in data.columns:\n",
        "            anomaly_days = data[data['is_anomaly'] == 1]\n",
        "            ax0.scatter(anomaly_days.index, anomaly_days['close'],\n",
        "                      color='red', label='Anomalies', zorder=5)\n",
        "\n",
        "        # Highlight manipulation if available\n",
        "        if 'predicted_manipulation' in data.columns:\n",
        "            manip_days = data[data['predicted_manipulation'] == 1]\n",
        "            ax0.scatter(manip_days.index, manip_days['close'],\n",
        "                      color='darkred', marker='X', s=100,\n",
        "                      label='Potential Manipulation', zorder=10)\n",
        "\n",
        "        ax0.set_ylabel('Price ($)')\n",
        "        ax0.legend()\n",
        "        ax0.grid(True, alpha=0.3)\n",
        "\n",
        "        # Volume plot\n",
        "        ax1 = axes[1]\n",
        "        ax1.set_title(f\"{self.ticker} Trading Volume\", fontsize=14)\n",
        "        ax1.bar(data.index, data['volume'], color='green', alpha=0.7, label='Volume')\n",
        "\n",
        "        # Highlight volume anomalies\n",
        "        volume_anomalies = data[data['volume_z_score'] > 1.5]\n",
        "        ax1.bar(volume_anomalies.index, volume_anomalies['volume'], color='orange', label='Volume Anomalies')\n",
        "\n",
        "        ax1.set_ylabel('Volume')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Sentiment plot\n",
        "        ax2 = axes[2]\n",
        "        ax2.set_title(\"Sentiment Analysis\", fontsize=14)\n",
        "\n",
        "        if 'tweet_sentiment_mean' in data.columns:\n",
        "            ax2.plot(data.index, data['tweet_sentiment_mean'],\n",
        "                   label='Social Sentiment', color='purple')\n",
        "\n",
        "        if 'news_sentiment_mean' in data.columns:\n",
        "            ax2.plot(data.index, data['news_sentiment_mean'],\n",
        "                   label='News Sentiment', color='brown')\n",
        "\n",
        "        # Add zero line\n",
        "        ax2.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
        "\n",
        "        ax2.set_ylabel('Sentiment Score')\n",
        "        ax2.set_xlabel('Date')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.ticker}_manipulation_analysis.png\")\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"\\nSaved visualization to {self.ticker}_manipulation_analysis.png\")\n",
        "\n",
        "def test_detector(ticker_symbol, demo_mode=True):\n",
        "    print(f\"\\n===== ANALYZING {ticker_symbol} =====\")\n",
        "    detector = StockManipulationDetector(ticker_symbol, lookback_days=365)\n",
        "    results = detector.detect_manipulation(demo_mode=True)\n",
        "    return results\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "\n",
        "    tickers = [\"NVDA\", \"AMD\", \"TSLA\", \"COIN\",\"GME\"]\n",
        "# - TSLA (Tesla)\n",
        "# - NVDA (NVIDIA)\n",
        "# - AMD (Advanced Micro Devices)\n",
        "# - COIN (Coinbase)\n",
        "# - GME (GameStop)\n",
        "    for ticker in tickers:\n",
        "        test_detector(ticker)\n",
        "        time.sleep(2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Smu9wAXnHdWa",
        "outputId": "e9c8d9b2-3b23-4132-d043-aaf643a5bbf6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-89e852d9a685>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Moved the following imports before using SentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/usr/local/share/nltk_data'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add nltk_data directory to the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Ensure nltk_data directory exists and contains the necessary resources.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[0;31m# Aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2475\u001b[0;31m \u001b[0m_downloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2476\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;31m# decide where we're going to save things to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# variety of system-wide locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W50s494IrdhO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}